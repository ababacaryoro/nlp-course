{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - Part 2: Text Preprocessing with NLTK\n",
    "\n",
    "**Course:** Natural Language Processing\n",
    "\n",
    "**Total Points:** 10 points (contributes to 50% of Assignment 1)\n",
    "\n",
    "---\n",
    "\n",
    "## Instructions\n",
    "\n",
    "1. Complete all the functions marked with `# YOUR CODE HERE`\n",
    "2. **DO NOT** change the function names or their signatures\n",
    "3. Each function must return the exact type specified\n",
    "4. Test your functions by running the test cells\n",
    "5. When finished:\n",
    "   - Export this notebook as a Python file (.py)\n",
    "   - **Name the file:** `LASTNAME_FIRSTNAME_assignment1_part2.py`\n",
    "   - Example: `DUPONT_Jean_assignment1_part2.py`\n",
    "   - Push to your GitHub repository\n",
    "   - Send the .py file by email to: **yoroba93@gmail.com**\n",
    "\n",
    "---\n",
    "\n",
    "## Assignment Overview\n",
    "\n",
    "In this assignment, you will use NLTK to analyze the Herman Melville novel **Moby Dick**.\n",
    "\n",
    "You will practice:\n",
    "- Tokenization\n",
    "- Frequency analysis\n",
    "- Stop word removal\n",
    "- Stemming and lemmatization\n",
    "- Building a preprocessing pipeline\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load the novel\n",
    "with open('moby.txt', 'r') as f:\n",
    "    moby_raw = f.read()\n",
    "\n",
    "# Create NLTK Text object\n",
    "moby_tokens = nltk.word_tokenize(moby_raw)\n",
    "text1 = nltk.Text(moby_tokens)\n",
    "\n",
    "print(f\"Loaded Moby Dick\")\n",
    "print(f\"Raw text length: {len(moby_raw)} characters\")\n",
    "print(f\"First 200 characters: {moby_raw[:200]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example Functions\n",
    "\n",
    "These examples show you how to work with the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Count total tokens\n",
    "def example_one():\n",
    "    return len(nltk.word_tokenize(moby_raw))\n",
    "\n",
    "print(f\"Total tokens: {example_one()}\")\n",
    "\n",
    "# Example 2: Count unique tokens\n",
    "def example_two():\n",
    "    return len(set(nltk.word_tokenize(moby_raw)))\n",
    "\n",
    "print(f\"Unique tokens: {example_two()}\")\n",
    "\n",
    "# Example 3: Lemmatize verbs and count unique\n",
    "def example_three():\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(w, 'v') for w in text1]\n",
    "    return len(set(lemmatized))\n",
    "\n",
    "print(f\"Unique tokens after verb lemmatization: {example_three()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 1 (1 point)\n",
    "\n",
    "**What is the lexical diversity of the text?**\n",
    "\n",
    "Lexical diversity = ratio of unique tokens to total number of tokens\n",
    "\n",
    "*This function should return a float.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_one():\n",
    "    \"\"\"\n",
    "    Calculate the lexical diversity of the text.\n",
    "    \n",
    "    Returns:\n",
    "        float: Ratio of unique tokens to total tokens\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Test your function\n",
    "q1_result = question_one()\n",
    "print(f\"Lexical diversity: {q1_result}\")\n",
    "# Expected: approximately 0.08 (8%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 2 (1 point)\n",
    "\n",
    "**What percentage of tokens is 'whale' or 'Whale'?**\n",
    "\n",
    "*This function should return a float (percentage, e.g., 0.5 for 0.5%).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_two():\n",
    "    \"\"\"\n",
    "    Calculate the percentage of tokens that are 'whale' or 'Whale'.\n",
    "    \n",
    "    Returns:\n",
    "        float: Percentage of whale tokens\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Test your function\n",
    "q2_result = question_two()\n",
    "print(f\"Percentage of 'whale'/'Whale': {q2_result}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 3 (1 point)\n",
    "\n",
    "**What are the 20 most frequently occurring (unique) tokens in the text? What is their frequency?**\n",
    "\n",
    "*This function should return a list of 20 tuples `(token, frequency)`, sorted in descending order of frequency.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_three():\n",
    "    \"\"\"\n",
    "    Find the 20 most frequent tokens and their frequencies.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of 20 tuples (token, frequency) sorted by frequency descending\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # Hint: Use nltk.FreqDist\n",
    "    \n",
    "    return []\n",
    "\n",
    "# Test your function\n",
    "q3_result = question_three()\n",
    "print(\"20 most frequent tokens:\")\n",
    "for token, freq in q3_result:\n",
    "    print(f\"  {token}: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 4 (1 point)\n",
    "\n",
    "**What tokens have a length greater than 5 and a frequency of more than 150?**\n",
    "\n",
    "*This function should return an alphabetically sorted list of tokens.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_four():\n",
    "    \"\"\"\n",
    "    Find tokens with length > 5 and frequency > 150.\n",
    "    \n",
    "    Returns:\n",
    "        list: Alphabetically sorted list of tokens\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return []\n",
    "\n",
    "# Test your function\n",
    "q4_result = question_four()\n",
    "print(f\"Found {len(q4_result)} tokens:\")\n",
    "print(q4_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 5 (1 point)\n",
    "\n",
    "**Find the longest word in text1 and its length.**\n",
    "\n",
    "*This function should return a tuple `(longest_word, length)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_five():\n",
    "    \"\"\"\n",
    "    Find the longest word in the text.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (longest_word, length)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return (None, 0)\n",
    "\n",
    "# Test your function\n",
    "q5_result = question_five()\n",
    "print(f\"Longest word: '{q5_result[0]}' with length {q5_result[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 6 (1 point)\n",
    "\n",
    "**What unique words (only alphabetic tokens) have a frequency of more than 2000?**\n",
    "\n",
    "Use `isalpha()` to check if the token is a word and not punctuation.\n",
    "\n",
    "*This function should return a list of tuples `(frequency, word)` sorted in descending order of frequency.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_six():\n",
    "    \"\"\"\n",
    "    Find words with frequency > 2000.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of tuples (frequency, word) sorted by frequency descending\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return []\n",
    "\n",
    "# Test your function\n",
    "q6_result = question_six()\n",
    "print(\"Words with frequency > 2000:\")\n",
    "for freq, word in q6_result:\n",
    "    print(f\"  {word}: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 7 (1 point)\n",
    "\n",
    "**What is the average number of tokens per sentence?**\n",
    "\n",
    "*This function should return a float.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_seven():\n",
    "    \"\"\"\n",
    "    Calculate the average number of tokens per sentence.\n",
    "    \n",
    "    Returns:\n",
    "        float: Average tokens per sentence\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # Hint: Use sent_tokenize for sentences, word_tokenize for words\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Test your function\n",
    "q7_result = question_seven()\n",
    "print(f\"Average tokens per sentence: {q7_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 8 (1 point)\n",
    "\n",
    "**Remove stop words from the text and return the 10 most common remaining words.**\n",
    "\n",
    "Only consider alphabetic tokens (use `isalpha()`).\n",
    "\n",
    "*This function should return a list of 10 tuples `(word, frequency)` sorted by frequency descending.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_eight():\n",
    "    \"\"\"\n",
    "    Find 10 most common words after removing stop words.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of 10 tuples (word, frequency) sorted by frequency descending\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # Hint: Use stopwords.words('english')\n",
    "    \n",
    "    return []\n",
    "\n",
    "# Test your function\n",
    "q8_result = question_eight()\n",
    "print(\"10 most common words (excluding stop words):\")\n",
    "for word, freq in q8_result:\n",
    "    print(f\"  {word}: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 9 (1 point)\n",
    "\n",
    "**Apply Porter stemming to all words and return the 10 most common stems.**\n",
    "\n",
    "Only consider alphabetic tokens.\n",
    "\n",
    "*This function should return a list of 10 tuples `(stem, frequency)` sorted by frequency descending.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_nine():\n",
    "    \"\"\"\n",
    "    Find 10 most common stems using Porter stemmer.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of 10 tuples (stem, frequency) sorted by frequency descending\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return []\n",
    "\n",
    "# Test your function\n",
    "q9_result = question_nine()\n",
    "print(\"10 most common stems:\")\n",
    "for stem, freq in q9_result:\n",
    "    print(f\"  {stem}: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 10 (1 point)\n",
    "\n",
    "**Create a complete preprocessing function that:**\n",
    "1. Tokenizes the text\n",
    "2. Converts to lowercase\n",
    "3. Removes non-alphabetic tokens\n",
    "4. Removes stop words\n",
    "5. Applies lemmatization\n",
    "\n",
    "Apply this function to the first 1000 characters of Moby Dick.\n",
    "\n",
    "*This function should return a list of preprocessed tokens.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_ten():\n",
    "    \"\"\"\n",
    "    Preprocess the first 1000 characters of Moby Dick.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of preprocessed tokens\n",
    "    \"\"\"\n",
    "    text = moby_raw[:1000]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # Steps:\n",
    "    # 1. Tokenize\n",
    "    # 2. Lowercase\n",
    "    # 3. Keep only alphabetic tokens\n",
    "    # 4. Remove stop words\n",
    "    # 5. Lemmatize\n",
    "    \n",
    "    return []\n",
    "\n",
    "# Test your function\n",
    "q10_result = question_ten()\n",
    "print(f\"Number of preprocessed tokens: {len(q10_result)}\")\n",
    "print(f\"First 20 tokens: {q10_result[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary of Functions for Grading\n",
    "\n",
    "Make sure all these functions are properly implemented before exporting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to verify all functions exist and return correct types\n",
    "print(\"Checking functions...\")\n",
    "\n",
    "try:\n",
    "    r1 = question_one()\n",
    "    assert isinstance(r1, float), \"question_one should return a float\"\n",
    "    print(\"✓ question_one: OK\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ question_one: {e}\")\n",
    "\n",
    "try:\n",
    "    r2 = question_two()\n",
    "    assert isinstance(r2, float), \"question_two should return a float\"\n",
    "    print(\"✓ question_two: OK\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ question_two: {e}\")\n",
    "\n",
    "try:\n",
    "    r3 = question_three()\n",
    "    assert isinstance(r3, list) and len(r3) == 20, \"question_three should return a list of 20 tuples\"\n",
    "    print(\"✓ question_three: OK\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ question_three: {e}\")\n",
    "\n",
    "try:\n",
    "    r4 = question_four()\n",
    "    assert isinstance(r4, list), \"question_four should return a list\"\n",
    "    print(\"✓ question_four: OK\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ question_four: {e}\")\n",
    "\n",
    "try:\n",
    "    r5 = question_five()\n",
    "    assert isinstance(r5, tuple) and len(r5) == 2, \"question_five should return a tuple of 2 elements\"\n",
    "    print(\"✓ question_five: OK\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ question_five: {e}\")\n",
    "\n",
    "try:\n",
    "    r6 = question_six()\n",
    "    assert isinstance(r6, list), \"question_six should return a list\"\n",
    "    print(\"✓ question_six: OK\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ question_six: {e}\")\n",
    "\n",
    "try:\n",
    "    r7 = question_seven()\n",
    "    assert isinstance(r7, float), \"question_seven should return a float\"\n",
    "    print(\"✓ question_seven: OK\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ question_seven: {e}\")\n",
    "\n",
    "try:\n",
    "    r8 = question_eight()\n",
    "    assert isinstance(r8, list) and len(r8) == 10, \"question_eight should return a list of 10 tuples\"\n",
    "    print(\"✓ question_eight: OK\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ question_eight: {e}\")\n",
    "\n",
    "try:\n",
    "    r9 = question_nine()\n",
    "    assert isinstance(r9, list) and len(r9) == 10, \"question_nine should return a list of 10 tuples\"\n",
    "    print(\"✓ question_nine: OK\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ question_nine: {e}\")\n",
    "\n",
    "try:\n",
    "    r10 = question_ten()\n",
    "    assert isinstance(r10, list), \"question_ten should return a list\"\n",
    "    print(\"✓ question_ten: OK\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ question_ten: {e}\")\n",
    "\n",
    "print(\"\\nDone! Export this notebook as .py file when all functions pass.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Submission Checklist\n",
    "\n",
    "- [ ] All 10 functions are implemented\n",
    "- [ ] All functions return the correct type\n",
    "- [ ] Notebook exported as Python file\n",
    "- [ ] File named: `LASTNAME_FIRSTNAME_assignment1_part2.py`\n",
    "- [ ] Pushed to GitHub repository\n",
    "- [ ] Sent to **yoroba93@gmail.com**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
