{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4 - Part 1: Core NLP Tasks\n",
    "\n",
    "**Course:** Natural Language Processing\n",
    "\n",
    "\n",
    "**Objectives:**\n",
    "- Apply Part-of-Speech (POS) tagging to extract linguistic patterns\n",
    "- Perform Named Entity Recognition (NER) to identify entities in text\n",
    "- Calculate word and document similarities using different techniques\n",
    "- Apply PCA for visualizing high-dimensional text representations\n",
    "- Work with real-world datasets (Nike products and legal contracts)\n",
    "\n",
    "---\n",
    "\n",
    "## Instructions\n",
    "\n",
    "1. Complete all exercises marked with `# YOUR CODE HERE`\n",
    "2. **Answer all written questions** in the designated markdown cells (these require YOUR personal interpretation)\n",
    "3. Save your completed notebook\n",
    "4. **Push to your Git repository and send the link to: yoroba93@gmail.com**\n",
    "\n",
    "### Important: Personal Interpretation Questions\n",
    "\n",
    "This lab contains **interpretation questions** that require YOUR own analysis. These questions:\n",
    "- Are based on YOUR specific results (which vary based on your choices)\n",
    "- Require you to explain your reasoning\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries (uncomment if needed)\n",
    "# !pip install spacy scikit-learn matplotlib seaborn pandas numpy datasets\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLP libraries\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"spaCy version: {spacy.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part A: Loading Nike Products Dataset\n",
    "\n",
    "We'll use the Nike product descriptions dataset to practice NLP tasks on commercial text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Nike products dataset\n",
    "# NOTE: Place the 'NikeProductDescriptions.csv' file in your working directory\n",
    "nike_df = pd.read_csv('NikeProductDescriptions.csv')\n",
    "\n",
    "print(f\"Dataset shape: {nike_df.shape}\")\n",
    "print(f\"\\nColumns: {nike_df.columns.tolist()}\")\n",
    "print(f\"\\nFirst 3 products:\")\n",
    "nike_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a sample product\n",
    "sample_idx = 0\n",
    "print(\"Sample Product:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Title: {nike_df.iloc[sample_idx]['Title']}\")\n",
    "print(f\"Subtitle: {nike_df.iloc[sample_idx]['Subtitle']}\")\n",
    "print(f\"\\nDescription:\\n{nike_df.iloc[sample_idx]['Product Description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part B: Part-of-Speech (POS) Tagging\n",
    "\n",
    "POS tagging identifies the grammatical role of each word (noun, verb, adjective, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: POS tagging with spaCy\n",
    "sample_text = \"Nike Air Force 1 shoes provide incredible comfort and stylish design for athletes.\"\n",
    "doc = nlp(sample_text)\n",
    "\n",
    "print(\"POS Tagging Example:\")\n",
    "print(\"=\" * 60)\n",
    "for token in doc:\n",
    "    print(f\"{token.text:15} | POS: {token.pos_:10} | Tag: {token.tag_:8} | Lemma: {token.lemma_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise B.1: Analyze POS Distribution in Nike Products\n",
    "\n",
    "Complete the function to extract and analyze POS tags from all Nike product descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_pos_distribution(texts):\n",
    "    \"\"\"\n",
    "    Analyze the distribution of POS tags in a list of texts.\n",
    "    \n",
    "    Args:\n",
    "        texts (list): List of text strings\n",
    "    \n",
    "    Returns:\n",
    "        Counter: Dictionary with POS tags and their counts\n",
    "    \"\"\"\n",
    "    pos_counts = Counter()\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # 1. For each text, process it with nlp(text)\n",
    "    # 2. For each token in the doc, count its POS tag (token.pos_)\n",
    "    # 3. Return the counter\n",
    "    \n",
    "    return pos_counts\n",
    "\n",
    "# Analyze Nike descriptions\n",
    "nike_descriptions = nike_df['Product Description'].dropna().tolist()\n",
    "pos_distribution = analyze_pos_distribution(nike_descriptions)\n",
    "\n",
    "print(\"POS Tag Distribution:\")\n",
    "print(\"=\" * 40)\n",
    "for pos, count in pos_distribution.most_common(15):\n",
    "    print(f\"{pos:10}: {count:5} ({count/sum(pos_distribution.values())*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize POS distribution\n",
    "top_pos = dict(pos_distribution.most_common(10))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(top_pos.keys(), top_pos.values(), color='steelblue')\n",
    "plt.xlabel('POS Tag', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Top 10 POS Tags in Nike Product Descriptions', fontsize=14)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('pos_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise B.2: Extract Adjectives and Verbs\n",
    "\n",
    "Marketing copy often uses powerful adjectives and action verbs. Extract the most common ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pos_words(texts, pos_tag, top_n=20):\n",
    "    \"\"\"\n",
    "    Extract words with a specific POS tag.\n",
    "    \n",
    "    Args:\n",
    "        texts (list): List of text strings\n",
    "        pos_tag (str): POS tag to extract (e.g., 'ADJ', 'VERB')\n",
    "        top_n (int): Number of top words to return\n",
    "    \n",
    "    Returns:\n",
    "        Counter: Most common words with the specified POS tag\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # 1. Process each text with spaCy\n",
    "    # 2. Extract tokens where token.pos_ == pos_tag\n",
    "    # 3. Use lemmatized form (token.lemma_.lower())\n",
    "    # 4. Filter out stopwords and short words (len < 3)\n",
    "    # 5. Return Counter with top_n most common\n",
    "    \n",
    "    return Counter(words).most_common(top_n)\n",
    "\n",
    "# Extract adjectives\n",
    "top_adjectives = extract_pos_words(nike_descriptions, 'ADJ', top_n=20)\n",
    "print(\"Top 20 Adjectives:\")\n",
    "print(\"=\" * 40)\n",
    "for word, count in top_adjectives:\n",
    "    print(f\"{word:15}: {count}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "\n",
    "# Extract verbs\n",
    "top_verbs = extract_pos_words(nike_descriptions, 'VERB', top_n=20)\n",
    "print(\"Top 20 Verbs:\")\n",
    "print(\"=\" * 40)\n",
    "for word, count in top_verbs:\n",
    "    print(f\"{word:15}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Written Question B.1 (Personal Interpretation)\n",
    "\n",
    "Analyze the linguistic patterns in Nike's marketing copy:\n",
    "\n",
    "1. **What do the most common adjectives reveal about Nike's brand messaging?** (List at least 3 adjectives and explain what they convey)\n",
    "2. **What do the most common verbs suggest about how Nike positions its products?** (List at least 3 verbs and their implications)\n",
    "3. **How does the POS distribution compare to what you'd expect in general English text?** (Consider the ratio of nouns/verbs/adjectives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER:**\n",
    "\n",
    "1. Key adjectives and brand messaging:\n",
    "   - ...\n",
    "   - ...\n",
    "   - ...\n",
    "\n",
    "2. Key verbs and product positioning:\n",
    "   - ...\n",
    "   - ...\n",
    "   - ...\n",
    "\n",
    "3. POS distribution comparison: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part C: Named Entity Recognition (NER)\n",
    "\n",
    "NER identifies and classifies named entities (people, organizations, locations, etc.) in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: NER with spaCy\n",
    "sample_text = \"Nike launched Air Jordan in 1984 in Chicago. Michael Jordan wore them throughout his NBA career.\"\n",
    "doc = nlp(sample_text)\n",
    "\n",
    "print(\"Named Entity Recognition Example:\")\n",
    "print(\"=\" * 60)\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text:20} | Type: {ent.label_:15} | Description: {spacy.explain(ent.label_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise C.1: Load Legal Contracts Dataset\n",
    "\n",
    "We'll use a sample of legal contracts to practice NER on more complex text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load a small sample of legal contracts (this dataset is very large!)\n",
    "# WARNING: Do NOT try to load the entire dataset - it will crash!\n",
    "print(\"Loading legal contracts dataset (sample only)...\")\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# Load only 50 examples from the 'train' split\n",
    "# Use: load_dataset(\"albertvillanova/legal_contracts\", split=\"train[:50]\")\n",
    "contracts_dataset = None  # Replace with your code\n",
    "\n",
    "# Convert to DataFrame\n",
    "contracts_df = pd.DataFrame(contracts_dataset)\n",
    "\n",
    "print(f\"Loaded {len(contracts_df)} contracts\")\n",
    "print(f\"\\nColumns: {contracts_df.columns.tolist()}\")\n",
    "print(f\"\\nFirst contract preview (first 500 chars):\")\n",
    "print(contracts_df.iloc[0]['text'][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise C.2: Extract and Analyze Named Entities\n",
    "\n",
    "Complete the function to extract entities from the legal contracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(texts, entity_types=None):\n",
    "    \"\"\"\n",
    "    Extract named entities from texts.\n",
    "    \n",
    "    Args:\n",
    "        texts (list): List of text strings\n",
    "        entity_types (list): List of entity types to extract (None = all types)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary with entity_type -> list of entities\n",
    "    \"\"\"\n",
    "    entities = defaultdict(list)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # 1. Process each text with spaCy\n",
    "    # 2. For each entity (doc.ents):\n",
    "    #    - If entity_types is None or entity.label_ in entity_types\n",
    "    #    - Add entity.text to entities[entity.label_]\n",
    "    # 3. Return entities dict\n",
    "    \n",
    "    return entities\n",
    "\n",
    "# Extract entities from contracts (process only first 10 for speed)\n",
    "contract_texts = contracts_df['text'].head(10).tolist()\n",
    "contract_entities = extract_entities(contract_texts)\n",
    "\n",
    "print(\"Entity Types Found:\")\n",
    "print(\"=\" * 60)\n",
    "for entity_type, entity_list in sorted(contract_entities.items()):\n",
    "    print(f\"\\n{entity_type} ({len(entity_list)} entities):\")\n",
    "    # Show unique entities only\n",
    "    unique_entities = Counter(entity_list).most_common(10)\n",
    "    for entity, count in unique_entities:\n",
    "        print(f\"  {entity}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise C.3: Compare Entity Distribution\n",
    "\n",
    "Compare the entity types found in Nike products vs. legal contracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# 1. Extract entities from Nike product descriptions\n",
    "# 2. Count entity types in both datasets\n",
    "# 3. Create a comparison visualization\n",
    "\n",
    "nike_entities = extract_entities(nike_descriptions[:50])  # Sample for speed\n",
    "\n",
    "# Count entity types\n",
    "nike_entity_counts = {etype: len(entities) for etype, entities in nike_entities.items()}\n",
    "contract_entity_counts = {etype: len(entities) for etype, entities in contract_entities.items()}\n",
    "\n",
    "# Get all entity types\n",
    "all_entity_types = set(list(nike_entity_counts.keys()) + list(contract_entity_counts.keys()))\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_data = []\n",
    "for etype in all_entity_types:\n",
    "    comparison_data.append({\n",
    "        'Entity Type': etype,\n",
    "        'Nike Products': nike_entity_counts.get(etype, 0),\n",
    "        'Legal Contracts': contract_entity_counts.get(etype, 0)\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('Legal Contracts', ascending=False)\n",
    "\n",
    "print(\"Entity Type Comparison:\")\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, comparison_df['Nike Products'], width, label='Nike Products', color='orange')\n",
    "ax.bar(x + width/2, comparison_df['Legal Contracts'], width, label='Legal Contracts', color='steelblue')\n",
    "\n",
    "ax.set_xlabel('Entity Type', fontsize=12)\n",
    "ax.set_ylabel('Count', fontsize=12)\n",
    "ax.set_title('Named Entity Distribution: Nike Products vs Legal Contracts', fontsize=14)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(comparison_df['Entity Type'], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('entity_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Written Question C.1 (Personal Interpretation)\n",
    "\n",
    "Analyze the differences in entity types between the two datasets:\n",
    "\n",
    "1. **Which entity types are most common in Nike products? Why does this make sense?**\n",
    "2. **Which entity types are most common in legal contracts? Why does this make sense?**\n",
    "3. **What does this tell you about the nature and purpose of each type of text?**\n",
    "4. **Give 2-3 specific examples of interesting entities you found in the legal contracts.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER:**\n",
    "\n",
    "1. Nike product entities: ...\n",
    "\n",
    "2. Legal contract entities: ...\n",
    "\n",
    "3. Text nature analysis: ...\n",
    "\n",
    "4. Interesting entities:\n",
    "   - ...\n",
    "   - ...\n",
    "   - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part D: Word and Document Similarities\n",
    "\n",
    "We'll explore different ways to measure similarity between words and documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise D.1: Word Similarity with spaCy Word Vectors\n",
    "\n",
    "spaCy's word vectors allow us to find semantically similar words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Find similar words\n",
    "def find_similar_words(word, top_n=10):\n",
    "    \"\"\"\n",
    "    Find words similar to the given word using spaCy word vectors.\n",
    "    \n",
    "    Args:\n",
    "        word (str): Input word\n",
    "        top_n (int): Number of similar words to return\n",
    "    \n",
    "    Returns:\n",
    "        list: List of (word, similarity_score) tuples\n",
    "    \"\"\"\n",
    "    word_doc = nlp(word)\n",
    "    \n",
    "    if not word_doc.has_vector:\n",
    "        return []\n",
    "    \n",
    "    # Get all words in spaCy's vocabulary that have vectors\n",
    "    similar_words = []\n",
    "    \n",
    "    # We'll check similarity with common words\n",
    "    for token in nlp.vocab:\n",
    "        if token.has_vector and token.is_lower and not token.is_stop:\n",
    "            similarity = word_doc.similarity(nlp(token.text))\n",
    "            similar_words.append((token.text, similarity))\n",
    "    \n",
    "    # Sort by similarity and return top_n (excluding the word itself)\n",
    "    similar_words.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [(w, s) for w, s in similar_words if w != word][:top_n]\n",
    "\n",
    "# Test with shoe-related words\n",
    "test_words = [\"running\", \"comfort\", \"athletic\", \"style\"]\n",
    "\n",
    "for word in test_words:\n",
    "    print(f\"\\nWords similar to '{word}':\")\n",
    "    print(\"=\" * 40)\n",
    "    similar = find_similar_words(word, top_n=8)\n",
    "    for similar_word, score in similar:\n",
    "        print(f\"  {similar_word:15}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise D.2: Document Similarity - Product Recommendations\n",
    "\n",
    "Build a simple product recommendation system using TF-IDF and cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_products(query_text, product_df, top_n=5):\n",
    "    \"\"\"\n",
    "    Find products most similar to a query text.\n",
    "    \n",
    "    Args:\n",
    "        query_text (str): Query description\n",
    "        product_df (DataFrame): DataFrame with product descriptions\n",
    "        top_n (int): Number of recommendations to return\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: Top similar products with similarity scores\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # 1. Create TF-IDF vectorizer\n",
    "    # 2. Fit on product descriptions + query\n",
    "    # 3. Transform all texts to TF-IDF vectors\n",
    "    # 4. Calculate cosine similarity between query and all products\n",
    "    # 5. Return top_n most similar products\n",
    "    \n",
    "    # Combine product descriptions with query\n",
    "    descriptions = product_df['Product Description'].tolist()\n",
    "    all_texts = descriptions + [query_text]\n",
    "    \n",
    "    # Create and fit vectorizer\n",
    "    vectorizer = TfidfVectorizer(max_features=500, stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
    "    \n",
    "    # Query vector is the last one\n",
    "    query_vector = tfidf_matrix[-1]\n",
    "    product_vectors = tfidf_matrix[:-1]\n",
    "    \n",
    "    # Calculate similarities\n",
    "    similarities = cosine_similarity(query_vector, product_vectors).flatten()\n",
    "    \n",
    "    # Get top_n indices\n",
    "    top_indices = similarities.argsort()[-top_n:][::-1]\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results = product_df.iloc[top_indices].copy()\n",
    "    results['Similarity'] = similarities[top_indices]\n",
    "    \n",
    "    return results[['Title', 'Subtitle', 'Similarity']]\n",
    "\n",
    "# Test with different queries\n",
    "queries = [\n",
    "    \"I want comfortable running shoes for long distance training\",\n",
    "    \"Looking for stylish basketball shoes with great cushioning\",\n",
    "    \"Need shoes for the gym and weight training\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    print(\"=\" * 80)\n",
    "    recommendations = find_similar_products(query, nike_df, top_n=5)\n",
    "    print(recommendations.to_string(index=False))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise D.3: Create YOUR Own Query\n",
    "\n",
    "Write your own custom query and analyze the recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Create your own query that reflects what YOU would look for in shoes\n",
    "my_query = \"___\"  # Write your custom query here\n",
    "\n",
    "print(f\"My Query: '{my_query}'\")\n",
    "print(\"=\" * 80)\n",
    "my_recommendations = find_similar_products(my_query, nike_df, top_n=5)\n",
    "print(my_recommendations.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Written Question D.1 (Personal Interpretation)\n",
    "\n",
    "Analyze the product recommendation results:\n",
    "\n",
    "1. **For YOUR custom query, are the top 3 recommendations relevant? Explain why or why not.**\n",
    "2. **Look at the similarity scores. What do you notice? Are they high, medium, or low? What does this mean?**\n",
    "3. **Compare the recommendations for \"running shoes\" vs \"basketball shoes\". What differences do you observe in the results?**\n",
    "4. **What are the limitations of this TF-IDF-based similarity approach? Give at least 2 specific limitations.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER:**\n",
    "\n",
    "1. Relevance of my recommendations:\n",
    "   - Top 1: ...\n",
    "   - Top 2: ...\n",
    "   - Top 3: ...\n",
    "\n",
    "2. Similarity scores observation: ...\n",
    "\n",
    "3. Running vs Basketball comparison: ...\n",
    "\n",
    "4. Limitations:\n",
    "   - ...\n",
    "   - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part E: Dimensionality Reduction with PCA (25 min)\n",
    "\n",
    "PCA helps us visualize high-dimensional text representations in 2D or 3D space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise E.1: Visualize Product Clusters\n",
    "\n",
    "Use PCA to create a 2D visualization of Nike products based on their descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# 1. Create TF-IDF vectors for all Nike product descriptions\n",
    "# 2. Apply PCA to reduce to 2 dimensions\n",
    "# 3. Create a scatter plot\n",
    "# 4. Color points by product category (extract from Subtitle)\n",
    "\n",
    "# Extract product descriptions\n",
    "descriptions = nike_df['Product Description'].tolist()\n",
    "\n",
    "# Create TF-IDF vectors\n",
    "vectorizer = TfidfVectorizer(max_features=200, stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(descriptions)\n",
    "\n",
    "print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
    "print(f\"Original dimensions: {tfidf_matrix.shape[1]}\")\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "pca_result = pca.fit_transform(tfidf_matrix.toarray())\n",
    "\n",
    "print(f\"\\nPCA explained variance ratio:\")\n",
    "print(f\"  PC1: {pca.explained_variance_ratio_[0]:.4f}\")\n",
    "print(f\"  PC2: {pca.explained_variance_ratio_[1]:.4f}\")\n",
    "print(f\"  Total: {sum(pca.explained_variance_ratio_):.4f}\")\n",
    "\n",
    "# Create DataFrame with PCA results\n",
    "pca_df = pd.DataFrame({\n",
    "    'PC1': pca_result[:, 0],\n",
    "    'PC2': pca_result[:, 1],\n",
    "    'Title': nike_df['Title'],\n",
    "    'Category': nike_df['Subtitle']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Get unique categories for coloring\n",
    "categories = pca_df['Category'].unique()\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(categories)))\n",
    "\n",
    "# Plot each category\n",
    "for i, category in enumerate(categories):\n",
    "    mask = pca_df['Category'] == category\n",
    "    plt.scatter(\n",
    "        pca_df.loc[mask, 'PC1'],\n",
    "        pca_df.loc[mask, 'PC2'],\n",
    "        c=[colors[i]],\n",
    "        label=category,\n",
    "        alpha=0.6,\n",
    "        s=100\n",
    "    )\n",
    "\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)', fontsize=12)\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)', fontsize=12)\n",
    "plt.title('Nike Products in 2D Space (PCA of TF-IDF)', fontsize=14)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('nike_products_pca.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise E.2: Find Products in Similar Regions\n",
    "\n",
    "Identify products that are close to each other in the PCA space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_neighbors_in_pca_space(product_index, pca_df, n_neighbors=5):\n",
    "    \"\"\"\n",
    "    Find products close to a given product in PCA space.\n",
    "    \n",
    "    Args:\n",
    "        product_index (int): Index of the reference product\n",
    "        pca_df (DataFrame): DataFrame with PCA coordinates\n",
    "        n_neighbors (int): Number of neighbors to find\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: Neighboring products with distances\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # 1. Get the PC1 and PC2 coordinates of the reference product\n",
    "    # 2. Calculate Euclidean distance to all other products\n",
    "    # 3. Return the n_neighbors closest products\n",
    "    \n",
    "    ref_point = pca_df.iloc[product_index][['PC1', 'PC2']].values\n",
    "    \n",
    "    # Calculate distances\n",
    "    distances = []\n",
    "    for idx, row in pca_df.iterrows():\n",
    "        if idx != product_index:\n",
    "            point = row[['PC1', 'PC2']].values\n",
    "            dist = np.linalg.norm(ref_point - point)\n",
    "            distances.append((idx, dist))\n",
    "    \n",
    "    # Sort by distance\n",
    "    distances.sort(key=lambda x: x[1])\n",
    "    \n",
    "    # Get neighbor indices\n",
    "    neighbor_indices = [idx for idx, _ in distances[:n_neighbors]]\n",
    "    neighbor_distances = [dist for _, dist in distances[:n_neighbors]]\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results = pca_df.iloc[neighbor_indices].copy()\n",
    "    results['Distance'] = neighbor_distances\n",
    "    \n",
    "    return results[['Title', 'Category', 'Distance']]\n",
    "\n",
    "# Test with a few products\n",
    "test_indices = [0, 10, 20]\n",
    "\n",
    "for idx in test_indices:\n",
    "    print(f\"\\nReference Product: {pca_df.iloc[idx]['Title']}\")\n",
    "    print(f\"Category: {pca_df.iloc[idx]['Category']}\")\n",
    "    print(\"=\" * 80)\n",
    "    neighbors = find_neighbors_in_pca_space(idx, pca_df, n_neighbors=5)\n",
    "    print(neighbors.to_string(index=False))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise E.3: Analyze Documents from Both Datasets\n",
    "\n",
    "Apply PCA to visualize both Nike products and legal contracts in the same space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# 1. Combine Nike descriptions and contract texts (sample 30 from each)\n",
    "# 2. Create TF-IDF vectors for combined corpus\n",
    "# 3. Apply PCA to reduce to 2D\n",
    "# 4. Create visualization with different colors for each dataset\n",
    "\n",
    "# Sample documents\n",
    "nike_sample = nike_df.sample(n=30, random_state=42)\n",
    "contracts_sample = contracts_df.sample(n=30, random_state=42)\n",
    "\n",
    "# Combine texts\n",
    "nike_texts = nike_sample['Product Description'].tolist()\n",
    "contract_texts = [text[:1000] for text in contracts_sample['text'].tolist()]  # Truncate for efficiency\n",
    "\n",
    "all_texts = nike_texts + contract_texts\n",
    "labels = ['Nike'] * len(nike_texts) + ['Contract'] * len(contract_texts)\n",
    "\n",
    "# Create TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=300, stop_words='english', max_df=0.8, min_df=2)\n",
    "tfidf_combined = vectorizer.fit_transform(all_texts)\n",
    "\n",
    "# Apply PCA\n",
    "pca_combined = PCA(n_components=2, random_state=42)\n",
    "pca_combined_result = pca_combined.fit_transform(tfidf_combined.toarray())\n",
    "\n",
    "# Create DataFrame\n",
    "combined_pca_df = pd.DataFrame({\n",
    "    'PC1': pca_combined_result[:, 0],\n",
    "    'PC2': pca_combined_result[:, 1],\n",
    "    'Dataset': labels\n",
    "})\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for dataset, color in [('Nike', 'orange'), ('Contract', 'steelblue')]:\n",
    "    mask = combined_pca_df['Dataset'] == dataset\n",
    "    plt.scatter(\n",
    "        combined_pca_df.loc[mask, 'PC1'],\n",
    "        combined_pca_df.loc[mask, 'PC2'],\n",
    "        c=color,\n",
    "        label=dataset,\n",
    "        alpha=0.6,\n",
    "        s=100\n",
    "    )\n",
    "\n",
    "plt.xlabel(f'PC1 ({pca_combined.explained_variance_ratio_[0]:.2%} variance)', fontsize=12)\n",
    "plt.ylabel(f'PC2 ({pca_combined.explained_variance_ratio_[1]:.2%} variance)', fontsize=12)\n",
    "plt.title('Nike Products vs Legal Contracts in 2D Space (PCA)', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('combined_datasets_pca.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"PCA explained variance:\")\n",
    "print(f\"  PC1: {pca_combined.explained_variance_ratio_[0]:.4f}\")\n",
    "print(f\"  PC2: {pca_combined.explained_variance_ratio_[1]:.4f}\")\n",
    "print(f\"  Total: {sum(pca_combined.explained_variance_ratio_):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Written Question E.1 (Personal Interpretation)\n",
    "\n",
    "Analyze the PCA visualizations:\n",
    "\n",
    "1. **Looking at the Nike products PCA plot:**\n",
    "   - Do similar product types cluster together?\n",
    "   - Can you identify any patterns or groups?\n",
    "   - What might the two principal components represent?\n",
    "\n",
    "2. **Looking at the combined Nike + Contracts PCA plot:**\n",
    "   - Are the two datasets clearly separated?\n",
    "   - What does this separation (or lack thereof) tell you?\n",
    "   - Are there any Nike products close to legal contracts? Why might this be?\n",
    "\n",
    "3. **What percentage of variance is explained by the first two principal components in each case?**\n",
    "   - Is this high or low?\n",
    "   - What does this mean for the quality of the 2D representation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER:**\n",
    "\n",
    "1. Nike products PCA analysis:\n",
    "   - Clustering: ...\n",
    "   - Patterns: ...\n",
    "   - PC interpretation: ...\n",
    "\n",
    "2. Combined datasets PCA analysis:\n",
    "   - Separation: ...\n",
    "   - Interpretation: ...\n",
    "   - Proximity cases: ...\n",
    "\n",
    "3. Variance explained:\n",
    "   - Nike products: ... (high/medium/low?)\n",
    "   - Combined: ... (high/medium/low?)\n",
    "   - Quality interpretation: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part F: Bonus Challenge - Dependency Parsing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Exercise: Visualize Sentence Structure\n",
    "\n",
    "Use spaCy's dependency parser to visualize grammatical relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# 1. Choose an interesting sentence from Nike or contracts\n",
    "# 2. Parse it with spaCy\n",
    "# 3. Visualize the dependency tree\n",
    "# 4. Identify the root verb, subjects, and objects\n",
    "\n",
    "sample_sentence = \"The Nike Air Force 1 combines classic style with modern comfort technology.\"\n",
    "\n",
    "doc = nlp(sample_sentence)\n",
    "\n",
    "# Display dependency visualization\n",
    "displacy.render(doc, style='dep', jupyter=True)\n",
    "\n",
    "# Print dependency information\n",
    "print(\"\\nDependency Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "for token in doc:\n",
    "    print(f\"{token.text:15} | HEAD: {token.head.text:15} | DEP: {token.dep_:10} | POS: {token.pos_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this lab, you learned:\n",
    "- **POS Tagging**: Identifying grammatical roles of words and analyzing linguistic patterns\n",
    "- **Named Entity Recognition (NER)**: Extracting and classifying entities like organizations, locations, and dates\n",
    "- **Word Similarity**: Using word vectors to find semantically similar words\n",
    "- **Document Similarity**: Building a product recommendation system with TF-IDF and cosine similarity\n",
    "- **PCA**: Visualizing high-dimensional text data in 2D space and discovering document clusters\n",
    "\n",
    "These are fundamental NLP tasks that power many real-world applications like:\n",
    "- Search engines and recommendation systems\n",
    "- Information extraction from documents\n",
    "- Text classification and clustering\n",
    "- Question answering systems\n",
    "\n",
    "---\n",
    "\n",
    "## Submission Checklist\n",
    "\n",
    "Before submitting, ensure you have:\n",
    "\n",
    "- [ ] Completed all exercises marked with `# YOUR CODE HERE`\n",
    "- [ ] Answered ALL written questions with YOUR personal interpretations\n",
    "- [ ] Generated all visualizations (POS distribution, entity comparison, PCA plots)\n",
    "- [ ] Saved visualization files (.png)\n",
    "- [ ] Tested your code (all cells run without errors)\n",
    "- [ ] Added meaningful comments to your code\n",
    "- [ ] Saved your notebook with outputs visible\n",
    "\n",
    "**Final Steps:**\n",
    "1. Save this notebook\n",
    "2. Push to your Git repository\n",
    "3. Send the repository link to: **yoroba93@gmail.com**\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
