{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 - Part 2: Word and Sentence Embeddings\n",
    "\n",
    "**Objectives:**\n",
    "- Understand and implement Word2Vec (CBOW and Skip-gram)\n",
    "- Work with pre-trained GloVe embeddings\n",
    "- Use BERT for sentence embeddings\n",
    "- Compare different embedding approaches\n",
    "- Apply embeddings to find similar words and documents\n",
    "\n",
    "---\n",
    "\n",
    "## Instructions\n",
    "\n",
    "1. Complete all exercises marked with `# YOUR CODE HERE`\n",
    "2. **Answer all written questions** in the designated markdown cells\n",
    "3. Save your completed notebook\n",
    "4. **Push to your Git repository and send the link to: yoroba93@gmail.com**\n",
    "\n",
    "### Important: This lab continues from Part 1\n",
    "\n",
    "You will use the same dataset and categories you chose in Part 1.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries (uncomment if needed)\n",
    "# !pip install gensim transformers torch sentence-transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import re\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import gensim.downloader as api\n",
    "\n",
    "print(f\"Gensim version: {gensim.__version__}\")\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset (Same as Part 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = # YOUR CODE HERE\n",
    "\n",
    "# TODO: Use the SAME 3 categories you chose in Part 1!\n",
    "my_categories = [\"___\", \"___\", \"___\"]  # COPY FROM PART 1\n",
    "\n",
    "# Filter dataset\n",
    "df_filtered = df[df['label_text'].isin(my_categories)].copy()\n",
    "df_filtered = df_filtered.reset_index(drop=True)\n",
    "\n",
    "print(f\"Selected categories: {my_categories}\")\n",
    "print(f\"Filtered dataset size: {len(df_filtered)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function (same as Part 1)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocess text for embedding training.\"\"\"\n",
    "    text = \"\" # YOUR CODE HERE  => the same as in Part 1 (advanced preprocessing)\n",
    "    tokens = []  # YOUR CODE HERE  => the same as in Part 1 (advanced preprocessing)\n",
    "    return tokens  # Return list of tokens for Word2Vec\n",
    "\n",
    "# Apply preprocessing\n",
    "df_filtered['tokens'] = df_filtered['text'].apply(preprocess_text)\n",
    "df_filtered['text_clean'] = df_filtered['tokens'].apply(' '.join)\n",
    "\n",
    "print(f\"Sample tokens: {df_filtered.iloc[0]['tokens'][:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part A: Word2Vec - Training Your Own Embeddings\n",
    "\n",
    "Word2Vec learns word representations by predicting context. There are two architectures:\n",
    "- **CBOW (Continuous Bag of Words)**: Predicts target word from context words\n",
    "- **Skip-gram**: Predicts context words from target word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.1 Understanding Word2Vec Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare corpus for Word2Vec (list of tokenized sentences)\n",
    "corpus = df_filtered['tokens'].tolist()\n",
    "\n",
    "print(f\"Corpus size: {len(corpus)} documents\")\n",
    "print(f\"Total tokens: {sum(len(doc) for doc in corpus)}\")\n",
    "print(f\"\\nSample document tokens: {corpus[0][:15]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec with CBOW (sg=0)\n",
    "model_cbow = Word2Vec(\n",
    "    sentences=corpus,\n",
    "    vector_size=100,      # Embedding dimension\n",
    "    window=5,             # Context window size\n",
    "    min_count=5,          # Ignore words with freq < 5\n",
    "    workers=4,            # Parallel threads\n",
    "    sg=0,                 # 0 = CBOW, 1 = Skip-gram\n",
    "    epochs=10             # Training epochs\n",
    ")\n",
    "\n",
    "print(f\"CBOW Model trained!\")\n",
    "print(f\"Vocabulary size: {len(model_cbow.wv)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec with Skip-gram (sg=1)\n",
    "model_skipgram = Word2Vec(\n",
    "    sentences=corpus,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=5,\n",
    "    workers=4,\n",
    "    sg=1,                 # Skip-gram\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "print(f\"Skip-gram Model trained!\")\n",
    "print(f\"Vocabulary size: {len(model_skipgram.wv)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.2 Exploring Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get word vector\n",
    "sample_word = \"computer\"  # Change this to a word relevant to YOUR categories\n",
    "\n",
    "if sample_word in model_cbow.wv:\n",
    "    vector = model_cbow.wv[sample_word]\n",
    "    print(f\"Vector for '{sample_word}':\")\n",
    "    print(f\"  Shape: {vector.shape}\")\n",
    "    print(f\"  First 10 values: {vector[:10]}\")\n",
    "else:\n",
    "    print(f\"'{sample_word}' not in vocabulary. Try another word.\")\n",
    "    print(f\"Sample words in vocab: {list(model_cbow.wv.key_to_index.keys())[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find similar words\n",
    "sample_word = \"computer\"  # Change to a word in YOUR vocabulary\n",
    "\n",
    "if sample_word in model_cbow.wv:\n",
    "    print(f\"\\nWords most similar to '{sample_word}' (CBOW):\")\n",
    "    for word, score in model_cbow.wv.most_similar(sample_word, topn=10):\n",
    "        print(f\"  {word}: {score:.4f}\")\n",
    "    \n",
    "    print(f\"\\nWords most similar to '{sample_word}' (Skip-gram):\")\n",
    "    for word, score in model_skipgram.wv.most_similar(sample_word, topn=10):\n",
    "        print(f\"  {word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise A.1: Compare CBOW vs Skip-gram\n",
    "\n",
    "Choose **5 words that are relevant to YOUR 3 categories** and compare the most similar words from both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Choose 5 words relevant to YOUR categories\n",
    "# These should be domain-specific words (not common words like \"good\", \"make\", etc.)\n",
    "\n",
    "my_test_words = [\"___\", \"___\", \"___\", \"___\", \"___\"]  # YOUR WORDS HERE\n",
    "\n",
    "comparison_results = []\n",
    "\n",
    "for word in my_test_words:\n",
    "    word = word.lower()\n",
    "    if word in model_cbow.wv and word in model_skipgram.wv:\n",
    "        cbow_similar = [w for w, s in model_cbow.wv.most_similar(word, topn=5)]\n",
    "        skipgram_similar = [w for w, s in model_skipgram.wv.most_similar(word, topn=5)]\n",
    "        \n",
    "        comparison_results.append({\n",
    "            'word': word,\n",
    "            'cbow_top5': cbow_similar,\n",
    "            'skipgram_top5': skipgram_similar\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n'{word}':\")\n",
    "        print(f\"  CBOW:     {cbow_similar}\")\n",
    "        print(f\"  Skip-gram: {skipgram_similar}\")\n",
    "    else:\n",
    "        print(f\"'{word}' not found in vocabulary!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Written Question A.1 (Personal Interpretation)\n",
    "\n",
    "Based on your comparison above:\n",
    "\n",
    "1. **For which words did CBOW and Skip-gram give SIMILAR results?**\n",
    "2. **For which words did they give DIFFERENT results?**\n",
    "3. **Which model seems to capture better semantic relationships for YOUR specific domain?** Explain with examples.\n",
    "4. **Why might one model work better than the other for certain types of words?** (Think about word frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER:**\n",
    "\n",
    "1. Similar results for: ...\n",
    "\n",
    "2. Different results for: ...\n",
    "\n",
    "3. Better model for my domain: ...\n",
    "   - Example 1: ...\n",
    "   - Example 2: ...\n",
    "\n",
    "4. Explanation of differences: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.3 Word Analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Word analogies (king - man + woman = queen)\n",
    "# This works better with larger, pre-trained models, but let's try with our custom model\n",
    "\n",
    "def find_analogy(model, word1, word2, word3):\n",
    "    \"\"\"\n",
    "    Find word that completes analogy: word1 is to word2 as word3 is to ?\n",
    "    Uses: word2 - word1 + word3 = ?\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = model.wv.most_similar(\n",
    "            positive=[word2, word3],\n",
    "            negative=[word1],\n",
    "            topn=5\n",
    "        )\n",
    "        return result\n",
    "    except KeyError as e:\n",
    "        return f\"Word not found: {e}\"\n",
    "\n",
    "# Test with your domain\n",
    "# Example: \"baseball\" is to \"bat\" as \"hockey\" is to ?\n",
    "print(\"Analogy test (your model may have limited vocabulary):\")\n",
    "# result = find_analogy(model_skipgram, \"word1\", \"word2\", \"word3\")\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise A.2: Create Domain-Specific Analogies\n",
    "\n",
    "Try to find **2 analogies** that work with YOUR dataset's vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try 2 analogies with words from YOUR vocabulary\n",
    "# Format: word1 is to word2 as word3 is to ?\n",
    "\n",
    "# Analogy 1\n",
    "# YOUR CODE HERE\n",
    "analogy1 = find_analogy(model_skipgram, \"___\", \"___\", \"___\")\n",
    "print(f\"Analogy 1: {analogy1}\")\n",
    "\n",
    "# Analogy 2\n",
    "# YOUR CODE HERE\n",
    "analogy2 = find_analogy(model_skipgram, \"___\", \"___\", \"___\")\n",
    "print(f\"Analogy 2: {analogy2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Written Question A.2 (Personal Interpretation)\n",
    "\n",
    "**Did your analogies work?** \n",
    "- If yes, explain why the result makes sense.\n",
    "- If no, explain why they might have failed (vocabulary size, training data, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER:**\n",
    "\n",
    "*[Analyze your analogy results]*\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part B: Pre-trained GloVe Embeddings \n",
    "\n",
    "GloVe (Global Vectors) is trained on much larger corpora and captures broader relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained GloVe embeddings (this may take a few minutes)\n",
    "print(\"Loading GloVe embeddings (this may take a minute)...\")\n",
    "glove_model = api.load('glove-wiki-gigaword-100')  # 100-dimensional vectors\n",
    "print(f\"GloVe loaded! Vocabulary size: {len(glove_model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare: Same word in YOUR model vs GloVe\n",
    "test_word = \"computer\"  # Change to a word relevant to your domain\n",
    "\n",
    "print(f\"Similar words to '{test_word}':\")\n",
    "print(\"\\nYour Word2Vec model:\")\n",
    "if test_word in model_skipgram.wv:\n",
    "    for word, score in model_skipgram.wv.most_similar(test_word, topn=10):\n",
    "        print(f\"  {word}: {score:.4f}\")\n",
    "else:\n",
    "    print(f\"  '{test_word}' not in vocabulary\")\n",
    "\n",
    "print(\"\\nPre-trained GloVe:\")\n",
    "if test_word in glove_model:\n",
    "    for word, score in glove_model.most_similar(test_word, topn=10):\n",
    "        print(f\"  {word}: {score:.4f}\")\n",
    "else:\n",
    "    print(f\"  '{test_word}' not in vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise B.1: Compare Your Model vs GloVe\n",
    "\n",
    "For **3 words from your domain**, compare the similar words from your trained model vs GloVe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare 3 domain-specific words\n",
    "\n",
    "comparison_words = [\"___\", \"___\", \"___\"]  # YOUR WORDS\n",
    "\n",
    "for word in comparison_words:\n",
    "    word = word.lower()\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Word: '{word}'\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Your model\n",
    "    print(\"Your Word2Vec:\")\n",
    "    if word in model_skipgram.wv:\n",
    "        for w, s in model_skipgram.wv.most_similar(word, topn=5):\n",
    "            print(f\"  {w}: {s:.3f}\")\n",
    "    else:\n",
    "        print(\"  Not in vocabulary\")\n",
    "    \n",
    "    # GloVe\n",
    "    print(\"GloVe:\")\n",
    "    if word in glove_model:\n",
    "        for w, s in glove_model.most_similar(word, topn=5):\n",
    "            print(f\"  {w}: {s:.3f}\")\n",
    "    else:\n",
    "        print(\"  Not in vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Written Question B.1 (Personal Interpretation)\n",
    "\n",
    "Compare your custom-trained Word2Vec model with pre-trained GloVe:\n",
    "\n",
    "1. **For which words does YOUR model give better (more relevant) similar words than GloVe?** Why?\n",
    "2. **For which words does GloVe give better results?** Why?\n",
    "3. **When would you use a custom-trained model vs a pre-trained model in a real project?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER:**\n",
    "\n",
    "1. My model is better for: ...\n",
    "   - Reason: ...\n",
    "\n",
    "2. GloVe is better for: ...\n",
    "   - Reason: ...\n",
    "\n",
    "3. When to use each:\n",
    "   - Custom model: ...\n",
    "   - Pre-trained model: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.2 GloVe Analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Famous analogy: king - man + woman = queen\n",
    "result = glove_model.most_similar(positive=['king', 'woman'], negative=['man'], topn=5)\n",
    "print(\"king - man + woman = ?\")\n",
    "for word, score in result:\n",
    "    print(f\"  {word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try 3 more analogies with GloVe\n",
    "# Be creative! Try analogies related to your categories.\n",
    "\n",
    "# Analogy 1: ___ is to ___ as ___ is to ?\n",
    "result1 = glove_model.most_similar(positive=['___', '___'], negative=['___'], topn=3)\n",
    "print(\"Analogy 1:\")\n",
    "print(result1)\n",
    "\n",
    "# Analogy 2\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Analogy 3\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part C: BERT Sentence Embeddings\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) creates contextual embeddings where the same word can have different representations based on context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load a pre-trained sentence transformer model\n",
    "print(\"Loading BERT-based sentence transformer...\")\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')  # Efficient model\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get sentence embeddings\n",
    "sample_sentences = [\n",
    "    \"I love programming in Python.\",\n",
    "    \"Python is my favorite programming language.\",\n",
    "    \"The python snake is very long.\",\n",
    "    \"I enjoy coding and software development.\"\n",
    "]\n",
    "\n",
    "# Encode sentences\n",
    "embeddings = sentence_model.encode(sample_sentences)\n",
    "\n",
    "print(f\"Embedding shape: {embeddings.shape}\")\n",
    "print(f\"Each sentence is represented by a {embeddings.shape[1]}-dimensional vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute sentence similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity = cosine_similarity(embeddings)\n",
    "\n",
    "print(\"Sentence similarity matrix:\")\n",
    "print(\"\\nSentences:\")\n",
    "for i, sent in enumerate(sample_sentences):\n",
    "    print(f\"  {i}: {sent}\")\n",
    "\n",
    "print(\"\\nSimilarity:\")\n",
    "sim_df = pd.DataFrame(similarity, \n",
    "                      index=[f\"S{i}\" for i in range(4)],\n",
    "                      columns=[f\"S{i}\" for i in range(4)])\n",
    "sim_df.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise C.1: Document Similarity with BERT\n",
    "\n",
    "Use BERT embeddings to find the most similar documents in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 30 documents (10 per category) for BERT embedding\n",
    "sampled_docs = []\n",
    "sampled_labels = []\n",
    "\n",
    "for category in my_categories:\n",
    "    cat_df = df_filtered[df_filtered['label_text'] == category].sample(n=10, random_state=42)\n",
    "    # Use first 500 characters of each document (BERT has length limits)\n",
    "    sampled_docs.extend(cat_df['text'].str[:500].tolist())\n",
    "    sampled_labels.extend([category] * 10)\n",
    "\n",
    "print(f\"Sampled {len(sampled_docs)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Encode documents with BERT and compute similarity matrix\n",
    "\n",
    "# Step 1: Encode all sampled documents\n",
    "doc_embeddings = None  # YOUR CODE HERE\n",
    "\n",
    "# Step 2: Compute cosine similarity\n",
    "bert_similarity = None  # YOUR CODE HERE\n",
    "\n",
    "print(f\"Similarity matrix shape: {bert_similarity.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize BERT similarity matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Create labels\n",
    "labels_short = [f\"{l[:6]}_{i%10}\" for i, l in enumerate(sampled_labels)]\n",
    "\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(\n",
    "    bert_similarity,\n",
    "    xticklabels=labels_short,\n",
    "    yticklabels=labels_short,\n",
    "    cmap='YlOrRd'\n",
    ")\n",
    "plt.title('Document Similarity (BERT Embeddings)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('bert_similarity_heatmap.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Written Question C.1 (Personal Interpretation)\n",
    "\n",
    "Compare the BERT similarity heatmap with the TF-IDF similarity heatmap from Part 1:\n",
    "\n",
    "1. **Do documents cluster better by category with BERT or TF-IDF?**\n",
    "2. **Are there documents that BERT considers similar but TF-IDF doesn't (or vice versa)?** Why might this happen?\n",
    "3. **Which method would you use for a document classification task?** Explain your reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER:**\n",
    "\n",
    "1. Better clustering with: ...\n",
    "\n",
    "2. Differences between methods: ...\n",
    "\n",
    "3. Preferred method for classification: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise C.2: Semantic Search with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a simple semantic search function\n",
    "# Given a query, find the most similar documents\n",
    "\n",
    "def semantic_search(query, documents, model, top_k=5):\n",
    "    \"\"\"\n",
    "    Find the most similar documents to a query using BERT embeddings.\n",
    "    \n",
    "    Args:\n",
    "        query (str): Search query\n",
    "        documents (list): List of document texts\n",
    "        model: Sentence transformer model\n",
    "        top_k (int): Number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        list: List of (index, similarity_score) tuples\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # 1. Encode the query\n",
    "    # 2. Compute similarity with all documents\n",
    "    # 3. Return top_k most similar\n",
    "    \n",
    "    return []\n",
    "\n",
    "# Test your search function\n",
    "# TODO: Write a query related to ONE of your categories\n",
    "my_query = \"___\"  # YOUR QUERY HERE\n",
    "\n",
    "results = semantic_search(my_query, sampled_docs, sentence_model, top_k=5)\n",
    "\n",
    "print(f\"Query: '{my_query}'\")\n",
    "print(\"\\nTop 5 most similar documents:\")\n",
    "for idx, score in results:\n",
    "    print(f\"\\n  Score: {score:.4f}\")\n",
    "    print(f\"  Category: {sampled_labels[idx]}\")\n",
    "    print(f\"  Text: {sampled_docs[idx][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Written Question C.2 (Personal Interpretation)\n",
    "\n",
    "Evaluate your semantic search results:\n",
    "\n",
    "1. **Are the results relevant to your query?** Explain.\n",
    "2. **Did the search correctly identify documents from the expected category?**\n",
    "3. **Try a query that could match multiple categories. What happens?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER:**\n",
    "\n",
    "1. Relevance: ...\n",
    "\n",
    "2. Category accuracy: ...\n",
    "\n",
    "3. Ambiguous query test: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part D: Embedding Visualization with t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Reduce BERT embeddings to 2D for visualization\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=10)\n",
    "embeddings_2d = tsne.fit_transform(doc_embeddings)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "colors = {'___': 'red', '___': 'blue', '___': 'green'}  # Update with your categories\n",
    "# Actually use your categories:\n",
    "color_map = plt.cm.Set1\n",
    "\n",
    "for i, category in enumerate(my_categories):\n",
    "    mask = [l == category for l in sampled_labels]\n",
    "    plt.scatter(\n",
    "        embeddings_2d[mask, 0],\n",
    "        embeddings_2d[mask, 1],\n",
    "        label=category,\n",
    "        alpha=0.7,\n",
    "        s=100\n",
    "    )\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Document Embeddings (BERT + t-SNE)')\n",
    "plt.xlabel('t-SNE dimension 1')\n",
    "plt.ylabel('t-SNE dimension 2')\n",
    "plt.tight_layout()\n",
    "plt.savefig('tsne_document_embeddings.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Written Question D.1 (Personal Interpretation)\n",
    "\n",
    "Look at your t-SNE visualization:\n",
    "\n",
    "1. **Do the categories form distinct clusters?**\n",
    "2. **Are there any documents that appear in the \"wrong\" cluster?** What might explain this?\n",
    "3. **Based on the visualization, which two categories are most similar?** Does this match your expectations from Part 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER:**\n",
    "\n",
    "1. Cluster quality: ...\n",
    "\n",
    "2. Misplaced documents: ...\n",
    "\n",
    "3. Most similar categories: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part E: Final Comparison and Reflection (10 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Written Question (Comprehensive Reflection)\n",
    "\n",
    "Based on everything you've learned in this lab:\n",
    "\n",
    "1. **Create a comparison table** summarizing the strengths and weaknesses of each text representation method:\n",
    "\n",
    "| Method | Strengths | Weaknesses | Best Use Case |\n",
    "|--------|-----------|------------|---------------|\n",
    "| BoW | ... | ... | ... |\n",
    "| TF-IDF | ... | ... | ... |\n",
    "| Word2Vec | ... | ... | ... |\n",
    "| GloVe | ... | ... | ... |\n",
    "| BERT | ... | ... | ... |\n",
    "\n",
    "2. **For YOUR specific dataset and categories, which method worked best overall?** Support your answer with specific evidence from your experiments.\n",
    "\n",
    "3. **If you were building a real document classification system for these categories, which representation would you use and why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER:**\n",
    "\n",
    "### 1. Comparison Table\n",
    "\n",
    "| Method | Strengths | Weaknesses | Best Use Case |\n",
    "|--------|-----------|------------|---------------|\n",
    "| BoW | ... | ... | ... |\n",
    "| TF-IDF | ... | ... | ... |\n",
    "| Word2Vec | ... | ... | ... |\n",
    "| GloVe | ... | ... | ... |\n",
    "| BERT | ... | ... | ... |\n",
    "\n",
    "### 2. Best Method for My Dataset\n",
    "\n",
    "*[Write at least 4-5 sentences with specific evidence]*\n",
    "\n",
    "...\n",
    "\n",
    "### 3. My Recommendation for a Real System\n",
    "\n",
    "*[Write your recommendation and justification]*\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary - Lab 3\n",
    "\n",
    "In this lab, you learned:\n",
    "\n",
    "**Part 1:**\n",
    "- Text visualization with bar charts and word clouds\n",
    "- Bag of Words and TF-IDF representations\n",
    "- N-grams and next-word prediction\n",
    "- Document correlation analysis\n",
    "\n",
    "**Part 2:**\n",
    "- Training Word2Vec models (CBOW vs Skip-gram)\n",
    "- Using pre-trained GloVe embeddings\n",
    "- BERT for sentence embeddings\n",
    "- Semantic search with embeddings\n",
    "- Embedding visualization with t-SNE\n",
    "\n",
    "---\n",
    "\n",
    "## Final Submission Checklist\n",
    "\n",
    "- [ ] All code exercises completed in Part 1 and Part 2\n",
    "- [ ] **All written questions answered with YOUR personal interpretation**\n",
    "- [ ] All visualizations saved (PNG files)\n",
    "- [ ] Both notebooks saved\n",
    "- [ ] Pushed to Git repository\n",
    "- [ ] **Repository link sent to: yoroba93@gmail.com**\n",
    "\n",
    "### Reminder: Oral Defense\n",
    "\n",
    "Be prepared to:\n",
    "- Explain your choice of categories and why\n",
    "- Discuss your written interpretations\n",
    "- Answer questions about the methods you used\n",
    "- Explain any surprising results you found"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
