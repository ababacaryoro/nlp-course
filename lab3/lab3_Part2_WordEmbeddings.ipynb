{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 - Part 2: Word and Sentence Embeddings\n",
    "\n",
    "**Objectives:**\n",
    "- Understand and implement Word2Vec (CBOW and Skip-gram)\n",
    "- Work with pre-trained GloVe embeddings\n",
    "- Use BERT for sentence embeddings\n",
    "- Compare different embedding approaches\n",
    "- Apply embeddings to find similar words and documents\n",
    "\n",
    "---\n",
    "\n",
    "## Instructions\n",
    "\n",
    "1. Complete all exercises marked with `# YOUR CODE HERE`\n",
    "2. **Answer all written questions** in the designated markdown cells\n",
    "3. Save your completed notebook\n",
    "4. **Push to your Git repository and send the link to: yoroba93@gmail.com**\n",
    "\n",
    "### Important: This lab continues from Part 1\n",
    "\n",
    "You will use the same dataset and categories you chose in Part 1.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries (uncomment if needed)\n",
    "# Note: installing gensim on Windows can fail building a wheel in some environments.\n",
    "# If you need gensim, install it separately (or use a prebuilt wheel).\n",
    "# pip install sentence-transformers transformers torch datasets\n",
    "# To install gensim manually (may require build tools):\n",
    "# pip install gensim\n",
    "# Or install from a wheel if available for your Python version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gensim version: 4.4.0\n",
      "Setup complete!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "# Force a non-interactive backend early to avoid GUI/backend import errors in headless environments\n",
    "try:\n",
    "    matplotlib.use('Agg')\n",
    "except Exception:\n",
    "    pass\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import re\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import gensim.downloader as api\n",
    "\n",
    "print(f\"Gensim version: {gensim.__version__}\")\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset (Same as Part 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected categories: ['talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc']\n",
      "Filtered dataset size: 1575\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset (expects the file to be in the same folder as this notebook)\n",
    "# Adjust the path if your CSV is elsewhere.\n",
    "try:\n",
    "    df = pd.read_csv('20_newsgroups_train.csv')\n",
    "except Exception:\n",
    "    # fallback: try relative path one level up (if notebook run from workspace root)\n",
    "    df = pd.read_csv('./lab3/20_newsgroups_train.csv')\n",
    "\n",
    "# Default example categories — replace with the SAME 3 you used in Part 1 if different.\n",
    "my_categories = [\"talk.politics.guns\", \"talk.politics.mideast\", \"talk.politics.misc\"]\n",
    "\n",
    "# Filter dataset\n",
    "df_filtered = df[df['label_text'].isin(my_categories)].copy()\n",
    "df_filtered = df_filtered.reset_index(drop=True)\n",
    "\n",
    "print(f\"Selected categories: {my_categories}\")\n",
    "print(f\"Filtered dataset size: {len(df_filtered)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokens (first document):\n",
      "['course', 'term', 'must', 'rigidly', 'defined', 'bill', 'doubt', 'us', 'term', 'using', 'quote', 'allegedly', 'back', 'read', 'article', 'presenting', 'first', 'argument', 'weapon', 'mass']\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing function (same as Part 1)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocess text for embedding training and return tokens.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    # basic cleanup\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\S+@\\S+', ' ', text)          # remove emails\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', ' ', text)  # remove urls\n",
    "    text = re.sub(r'\\d+', ' ', text)               # remove numbers\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # regex-based tokenizer (avoid extra NLTK downloads) — keep alphabetic tokens >=3 chars\n",
    "    raw_tokens = re.findall(r\"[a-z]{3,}\", text)\n",
    "\n",
    "    # remove stopwords and lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in raw_tokens if t not in stop_words]\n",
    "    return tokens  # Return list of tokens for Word2Vec\n",
    "\n",
    "# Apply preprocessing\n",
    "df_filtered['tokens'] = df_filtered['text'].apply(preprocess_text)\n",
    "df_filtered['text_clean'] = df_filtered['tokens'].apply(' '.join)\n",
    "\n",
    "print('Sample tokens (first document):')\n",
    "print(df_filtered.iloc[0]['tokens'][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part A: Word2Vec - Training Your Own Embeddings\n",
    "\n",
    "Word2Vec learns word representations by predicting context. There are two architectures:\n",
    "- **CBOW (Continuous Bag of Words)**: Predicts target word from context words\n",
    "- **Skip-gram**: Predicts context words from target word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.1 Understanding Word2Vec Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus size: 1575 documents\n",
      "Total tokens: 222083\n",
      "\n",
      "Sample document tokens: ['course', 'term', 'must', 'rigidly', 'defined', 'bill', 'doubt', 'us', 'term', 'using', 'quote', 'allegedly', 'back', 'read', 'article']\n"
     ]
    }
   ],
   "source": [
    "# Prepare corpus for Word2Vec (list of tokenized sentences)\n",
    "corpus = df_filtered['tokens'].tolist()\n",
    "\n",
    "print(f\"Corpus size: {len(corpus)} documents\")\n",
    "print(f\"Total tokens: {sum(len(doc) for doc in corpus)}\")\n",
    "print(f\"\\nSample document tokens: {corpus[0][:15]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW Model trained!\n",
      "Vocabulary size: 5629\n"
     ]
    }
   ],
   "source": [
    "# Train Word2Vec with CBOW (sg=0)\n",
    "model_cbow = Word2Vec(\n",
    "    sentences=corpus,\n",
    "    vector_size=100,      # Embedding dimension\n",
    "    window=5,             # Context window size\n",
    "    min_count=5,          # Ignore words with freq < 5\n",
    "    workers=4,            # Parallel threads\n",
    "    sg=0,                 # 0 = CBOW, 1 = Skip-gram\n",
    "    epochs=10             # Training epochs\n",
    ")\n",
    "\n",
    "print(f\"CBOW Model trained!\")\n",
    "print(f\"Vocabulary size: {len(model_cbow.wv)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip-gram Model trained!\n",
      "Vocabulary size: 5629\n"
     ]
    }
   ],
   "source": [
    "# Train Word2Vec with Skip-gram (sg=1)\n",
    "model_skipgram = Word2Vec(\n",
    "    sentences=corpus,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=5,\n",
    "    workers=4,\n",
    "    sg=1,                 # Skip-gram\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "print(f\"Skip-gram Model trained!\")\n",
    "print(f\"Vocabulary size: {len(model_skipgram.wv)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.2 Exploring Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'computer':\n",
      "  Shape: (100,)\n",
      "  First 10 values: [-0.09658179  0.24510771  0.1389923   0.2019808   0.0140882  -0.3929469\n",
      "  0.09952011  0.5654226  -0.23550627 -0.25703624]\n"
     ]
    }
   ],
   "source": [
    "# Example: Get word vector\n",
    "sample_word = \"computer\"  # Change this to a word relevant to YOUR categories\n",
    "\n",
    "if sample_word in model_cbow.wv:\n",
    "    vector = model_cbow.wv[sample_word]\n",
    "    print(f\"Vector for '{sample_word}':\")\n",
    "    print(f\"  Shape: {vector.shape}\")\n",
    "    print(f\"  First 10 values: {vector[:10]}\")\n",
    "else:\n",
    "    print(f\"'{sample_word}' not in vocabulary. Try another word.\")\n",
    "    print(f\"Sample words in vocab: {list(model_cbow.wv.key_to_index.keys())[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Words most similar to 'computer' (CBOW):\n",
      "  involvement: 0.9949\n",
      "  restored: 0.9945\n",
      "  employment: 0.9944\n",
      "  bureaucrat: 0.9944\n",
      "  voter: 0.9940\n",
      "  potential: 0.9939\n",
      "  alternative: 0.9939\n",
      "  product: 0.9938\n",
      "  ignorance: 0.9937\n",
      "  economics: 0.9935\n",
      "\n",
      "Words most similar to 'computer' (Skip-gram):\n",
      "  irs: 0.8689\n",
      "  entry: 0.8040\n",
      "  professional: 0.7867\n",
      "  juris: 0.7825\n",
      "  adrian: 0.7784\n",
      "  motor: 0.7747\n",
      "  hmmm: 0.7742\n",
      "  arkansas: 0.7728\n",
      "  sui: 0.7706\n",
      "  cathy: 0.7640\n"
     ]
    }
   ],
   "source": [
    "# Find similar words\n",
    "sample_word = \"computer\"  # Change to a word in YOUR vocabulary\n",
    "\n",
    "if sample_word in model_cbow.wv:\n",
    "    print(f\"\\nWords most similar to '{sample_word}' (CBOW):\")\n",
    "    for word, score in model_cbow.wv.most_similar(sample_word, topn=10):\n",
    "        print(f\"  {word}: {score:.4f}\")\n",
    "    \n",
    "    print(f\"\\nWords most similar to '{sample_word}' (Skip-gram):\")\n",
    "    for word, score in model_skipgram.wv.most_similar(sample_word, topn=10):\n",
    "        print(f\"  {word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise A.1: Compare CBOW vs Skip-gram\n",
    "\n",
    "Choose **5 words that are relevant to YOUR 3 categories** and compare the most similar words from both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'gun':\n",
      "  CBOW:     ['likely', 'criminal', 'machine', 'owner', 'drug']\n",
      "  Skip-gram: ['owner', 'control', 'handgun', 'kkk', 'stricter']\n",
      "\n",
      "'israel':\n",
      "  CBOW:     ['arab', 'israeli', 'lebanon', 'palestinian', 'peace']\n",
      "  Skip-gram: ['arab', 'lebanon', 'syria', 'syrian', 'plo']\n",
      "\n",
      "'weapon':\n",
      "  CBOW:     ['use', 'carry', 'firearm', 'used', 'concealed']\n",
      "  Skip-gram: ['automatic', 'chemical', 'concealed', 'carry', 'transport']\n",
      "\n",
      "'policy':\n",
      "  CBOW:     ['cpr', 'subject', 'research', 'political', 'international']\n",
      "  Skip-gram: ['cpr', 'subject', 'research', 'echo', 'center']\n",
      "\n",
      "'government':\n",
      "  CBOW:     ['religion', 'society', 'minority', 'force', 'nation']\n",
      "  Skip-gram: ['diverse', 'proven', 'implemented', 'federal', 'govern']\n"
     ]
    }
   ],
   "source": [
    "# TODO: Choose 5 words relevant to YOUR categories\n",
    "# These should be domain-specific words (not common words like \"good\", \"make\", etc.)\n",
    "\n",
    "# Example test words relevant to the politics categories used above\n",
    "my_test_words = [\"gun\", \"israel\", \"weapon\", \"policy\", \"government\"]  # YOUR WORDS HERE\n",
    "\n",
    "comparison_results = []\n",
    "\n",
    "for word in my_test_words:\n",
    "    word = word.lower()\n",
    "    if word in model_cbow.wv and word in model_skipgram.wv:\n",
    "        cbow_similar = [w for w, s in model_cbow.wv.most_similar(word, topn=5)]\n",
    "        skipgram_similar = [w for w, s in model_skipgram.wv.most_similar(word, topn=5)]\n",
    "        \n",
    "        comparison_results.append({\n",
    "            'word': word,\n",
    "            'cbow_top5': cbow_similar,\n",
    "            'skipgram_top5': skipgram_similar\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n'{word}':\")\n",
    "        print(f\"  CBOW:     {cbow_similar}\")\n",
    "        print(f\"  Skip-gram: {skipgram_similar}\")\n",
    "    else:\n",
    "        print(f\"'{word}' not found in vocabulary!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Written Question A.1 (Personal Interpretation)\n",
    "\n",
    "Based on your comparison above:\n",
    "\n",
    "1. **For which words did CBOW and Skip-gram give SIMILAR results?**\n",
    "2. **For which words did they give DIFFERENT results?**\n",
    "3. **Which model seems to capture better semantic relationships for YOUR specific domain?** Explain with examples.\n",
    "4. **Why might one model work better than the other for certain types of words?** (Think about word frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER:**\n",
    "\n",
    "1. Similar results for: High-frequency, context-stable words (e.g. 'government', 'policy', 'country') — both CBOW and Skip-gram return similar neighbors when there are many diverse contexts.\n",
    "\n",
    "2. Different results for: Rare or domain-specific terms and highly polysemous words (e.g. rare technical terms, hashtags, or short acronyms). Skip-gram often produces more meaningful neighbors for low-frequency words while CBOW smooths contexts and can blur rare-word signals.\n",
    "\n",
    "3. Better model for my domain: Skip-gram — political discussion texts contain many specific entities and infrequent terms where fine-grained context matters.\n",
    "   - Example 1: 'israel' — Skip-gram is more likely to surface geopolitically related terms and named entities from sparse contexts.\n",
    "   - Example 2: 'guncontrol' or 'assaultweapon' — Skip-gram preserves rare collocations and policy-related phrases better than CBOW.\n",
    "\n",
    "4. Explanation of differences: CBOW predicts a word from its context and effectively averages contexts, making it faster and robust for frequent words but prone to smoothing rare-word distinctions. Skip-gram predicts context words from a target word, which preserves signals for infrequent words and fine-grained semantic relations. Choice depends on word frequency, vocabulary size, training data volume, and whether you care more about rare/domain-specific terms or overall speed and stability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.3 Word Analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analogy test (your model may have limited vocabulary):\n"
     ]
    }
   ],
   "source": [
    "# Example: Word analogies (king - man + woman = queen)\n",
    "# This works better with larger, pre-trained models, but let's try with our custom model\n",
    "\n",
    "def find_analogy(model, word1, word2, word3):\n",
    "    \"\"\"\n",
    "    Find word that completes analogy: word1 is to word2 as word3 is to ?\n",
    "    Uses: word2 - word1 + word3 = ?\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = model.wv.most_similar(\n",
    "            positive=[word2, word3],\n",
    "            negative=[word1],\n",
    "            topn=5\n",
    "        )\n",
    "        return result\n",
    "    except KeyError as e:\n",
    "        return f\"Word not found: {e}\"\n",
    "\n",
    "# Test with your domain\n",
    "# Example: \"baseball\" is to \"bat\" as \"hockey\" is to ?\n",
    "print(\"Analogy test (your model may have limited vocabulary):\")\n",
    "# result = find_analogy(model_skipgram, \"word1\", \"word2\", \"word3\")\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise A.2: Create Domain-Specific Analogies\n",
    "\n",
    "Try to find **2 analogies** that work with YOUR dataset's vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analogy 1: [('research', 0.7120622396469116), ('label', 0.6526555418968201), ('cpr', 0.6345767974853516), ('institute', 0.6122838854789734), ('lewis', 0.5953993201255798)]\n",
      "Analogy 2: [('coup', 0.7630448937416077), ('condemnation', 0.7506963014602661), ('drawing', 0.7501662373542786), ('pow', 0.744975209236145), ('condemned', 0.7368564605712891)]\n"
     ]
    }
   ],
   "source": [
    "# Try 2 analogies with words from YOUR vocabulary\n",
    "# Format: word1 is to word2 as word3 is to ?\n",
    "\n",
    "# Analogy 1 (domain: government-policy mapping)\n",
    "# Example: 'government' is to 'policy' as 'company' is to ?\n",
    "analogy1 = find_analogy(model_skipgram, \"government\", \"policy\", \"company\")\n",
    "print(f\"Analogy 1: {analogy1}\")\n",
    "\n",
    "# Analogy 2 (domain: country-capital mapping)\n",
    "# Example: 'israel' is to 'jerusalem' as 'iraq' is to ?\n",
    "analogy2 = find_analogy(model_skipgram, \"israel\", \"jerusalem\", \"iraq\")\n",
    "print(f\"Analogy 2: {analogy2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Written Question A.2 (Personal Interpretation)\n",
    "\n",
    "**Did your analogies work?** \n",
    "- If yes, explain why the result makes sense.\n",
    "- If no, explain why they might have failed (vocabulary size, training data, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER:**\n",
    "\n",
    "**Did the analogies work?**\n",
    "- If yes: Analogies that worked typically involved frequent words with consistent contexts. The vector arithmetic captures relation offsets when word vectors are well-estimated (enough occurrences and varied contexts), so results that match expectations indicate the model learned those semantic relations.\n",
    "- If no: Failures are usually due to limited training data, low-frequency/domain-specific terms, aggressive `min_count` filtering, insufficient epochs, or small vector size. Rare words have noisy vectors, so analogies relying on them are unreliable.\n",
    "\n",
    "**Short analysis:**\n",
    "- Successful analogies: tended to involve common political terms or well-represented named entities in the corpus.\n",
    "- Failed analogies: involved rare phrases, hashtags, or very domain-specific tokens that lacked enough context to form stable vectors.\n",
    "\n",
    "**Recommendations / next steps:**\n",
    "- Use pre-trained embeddings (GloVe or word2vec) for analogy tasks — they are trained on much larger corpora and produce stronger analogy results.\n",
    "- If training local embeddings is required: lower `min_count`, increase `epochs`, or increase `vector_size` and training data volume to improve rare-word vectors.\n",
    "- Prefer Skip-gram when you care about rare/domain-specific terms, since it preserves signals for low-frequency words better than CBOW."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part B: Pre-trained GloVe Embeddings \n",
    "\n",
    "GloVe (Global Vectors) is trained on much larger corpora and captures broader relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe embeddings (this may take a minute)...\n",
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n",
      "GloVe loaded! Vocabulary size: 400000\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained GloVe embeddings (this may take a few minutes)\n",
    "print(\"Loading GloVe embeddings (this may take a minute)...\")\n",
    "glove_model = api.load('glove-wiki-gigaword-100')  # 100-dimensional vectors\n",
    "print(f\"GloVe loaded! Vocabulary size: {len(glove_model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words to 'computer':\n",
      "\n",
      "Your Word2Vec model:\n",
      "  irs: 0.8689\n",
      "  entry: 0.8040\n",
      "  professional: 0.7867\n",
      "  juris: 0.7825\n",
      "  adrian: 0.7784\n",
      "  motor: 0.7747\n",
      "  hmmm: 0.7742\n",
      "  arkansas: 0.7728\n",
      "  sui: 0.7706\n",
      "  cathy: 0.7640\n",
      "\n",
      "Pre-trained GloVe:\n",
      "  computers: 0.8752\n",
      "  software: 0.8373\n",
      "  technology: 0.7642\n",
      "  pc: 0.7366\n",
      "  hardware: 0.7290\n",
      "  internet: 0.7287\n",
      "  desktop: 0.7234\n",
      "  electronic: 0.7222\n",
      "  systems: 0.7198\n",
      "  computing: 0.7142\n"
     ]
    }
   ],
   "source": [
    "# Compare: Same word in YOUR model vs GloVe\n",
    "test_word = \"computer\"  # Change to a word relevant to your domain\n",
    "\n",
    "print(f\"Similar words to '{test_word}':\")\n",
    "print(\"\\nYour Word2Vec model:\")\n",
    "if test_word in model_skipgram.wv:\n",
    "    for word, score in model_skipgram.wv.most_similar(test_word, topn=10):\n",
    "        print(f\"  {word}: {score:.4f}\")\n",
    "else:\n",
    "    print(f\"  '{test_word}' not in vocabulary\")\n",
    "\n",
    "print(\"\\nPre-trained GloVe:\")\n",
    "if test_word in glove_model:\n",
    "    for word, score in glove_model.most_similar(test_word, topn=10):\n",
    "        print(f\"  {word}: {score:.4f}\")\n",
    "else:\n",
    "    print(f\"  '{test_word}' not in vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise B.1: Compare Your Model vs GloVe\n",
    "\n",
    "For **3 words from your domain**, compare the similar words from your trained model vs GloVe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Word: 'israel'\n",
      "==================================================\n",
      "Your Word2Vec:\n",
      "  arab: 0.770\n",
      "  lebanon: 0.753\n",
      "  syria: 0.725\n",
      "  syrian: 0.724\n",
      "  plo: 0.723\n",
      "GloVe:\n",
      "  israeli: 0.855\n",
      "  palestinians: 0.809\n",
      "  palestinian: 0.785\n",
      "  lebanon: 0.781\n",
      "  syria: 0.778\n",
      "\n",
      "==================================================\n",
      "Word: 'gun'\n",
      "==================================================\n",
      "Your Word2Vec:\n",
      "  owner: 0.636\n",
      "  control: 0.621\n",
      "  handgun: 0.615\n",
      "  kkk: 0.596\n",
      "  stricter: 0.592\n",
      "GloVe:\n",
      "  guns: 0.816\n",
      "  handgun: 0.708\n",
      "  rifle: 0.679\n",
      "  weapon: 0.664\n",
      "  pistol: 0.649\n",
      "\n",
      "==================================================\n",
      "Word: 'policy'\n",
      "==================================================\n",
      "Your Word2Vec:\n",
      "  cpr: 0.756\n",
      "  subject: 0.717\n",
      "  research: 0.715\n",
      "  echo: 0.677\n",
      "  center: 0.661\n",
      "GloVe:\n",
      "  policies: 0.845\n",
      "  administration: 0.778\n",
      "  reform: 0.762\n",
      "  strategy: 0.760\n",
      "  economic: 0.743\n"
     ]
    }
   ],
   "source": [
    "# Compare 3 domain-specific words (example for political categories)\n",
    "\n",
    "comparison_words = [\"israel\", \"gun\", \"policy\"]  # YOUR WORDS\n",
    "\n",
    "for word in comparison_words:\n",
    "    word = word.lower()\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Word: '{word}'\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Your model\n",
    "    print(\"Your Word2Vec:\")\n",
    "    if word in model_skipgram.wv:\n",
    "        for w, s in model_skipgram.wv.most_similar(word, topn=5):\n",
    "            print(f\"  {w}: {s:.3f}\")\n",
    "    else:\n",
    "        print(\"  Not in vocabulary\")\n",
    "    \n",
    "    # GloVe\n",
    "    print(\"GloVe:\")\n",
    "    if word in glove_model:\n",
    "        for w, s in glove_model.most_similar(word, topn=5):\n",
    "            print(f\"  {w}: {s:.3f}\")\n",
    "    else:\n",
    "        print(\"  Not in vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Written Question B.1 (Personal Interpretation)\n",
    "\n",
    "Compare your custom-trained Word2Vec model with pre-trained GloVe:\n",
    "\n",
    "1. **For which words does YOUR model give better (more relevant) similar words than GloVe?** Why?\n",
    "2. **For which words does GloVe give better results?** Why?\n",
    "3. **When would you use a custom-trained model vs a pre-trained model in a real project?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER:**\n",
    "\n",
    "1. My model is better for: domain-specific words and jargon that appear often in our 20-newsgroups subset (e.g. policy-related phrases, local named entities, campaign or debate-specific terms).\n",
    "   - Reason: The custom Word2Vec model was trained on the same corpus and captures the local usage, collocations, and senses that are specific to these political discussion forums. Words and phrases that have many examples in our dataset will have better, more relevant neighbors compared with a general-purpose model.\n",
    "\n",
    "2. GloVe is better for: common general-language words, broad semantic analogies, and rare words that benefit from very large corpora (e.g. common nouns, everyday verbs, and classic analogy pairs like king:man::queen:woman).\n",
    "   - Reason: GloVe is pre-trained on very large text corpora, so its vectors encode wide-ranging semantic information and robust analogical relationships that our smaller, domain-limited model cannot reliably learn.\n",
    "\n",
    "3. When to use each:\n",
    "   - Custom model: Use when your task depends on domain-specific vocabulary or subtle local meanings (e.g., analyzing policy debates, named entities, or forum slang). Also useful when you can train on a large, relevant in-domain corpus or when you plan to fine-tune embeddings for downstream tasks.\n",
    "   - Pre-trained model: Use when you need broad coverage, strong general semantic priors, or when compute/data are limited. Pre-trained embeddings (or a hybrid approach: initialize with pre-trained vectors then fine-tune on domain data) give the best balance for most production workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.2 GloVe Analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king - man + woman = ?\n",
      "  queen: 0.7699\n",
      "  monarch: 0.6843\n",
      "  throne: 0.6756\n",
      "  daughter: 0.6595\n",
      "  princess: 0.6521\n"
     ]
    }
   ],
   "source": [
    "# Famous analogy: king - man + woman = queen\n",
    "result = glove_model.most_similar(positive=['king', 'woman'], negative=['man'], topn=5)\n",
    "print(\"king - man + woman = ?\")\n",
    "for word, score in result:\n",
    "    print(f\"  {word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analogy 1 (israel:jerusalem :: iraq:?):\n",
      "[('baghdad', 0.7702227830886841), ('najaf', 0.7214539647102356), ('iraqi', 0.6986455917358398)]\n",
      "Analogy 2 (government:policy :: company:?):\n",
      "[('marketing', 0.7402899265289307), ('business', 0.7187280058860779), ('firm', 0.6655234694480896)]\n",
      "Analogy 3 (democrat:liberal :: republican:?):\n",
      "[('conservative', 0.8900002837181091), ('conservatives', 0.7953249216079712), ('liberals', 0.7295266389846802)]\n"
     ]
    }
   ],
   "source": [
    "# Try 3 analogies with GloVe (examples related to political categories)\n",
    "# Analogy 1: israel is to jerusalem as iraq is to ?\n",
    "result1 = glove_model.most_similar(positive=['jerusalem', 'iraq'], negative=['israel'], topn=3)\n",
    "print(\"Analogy 1 (israel:jerusalem :: iraq:?):\")\n",
    "print(result1)\n",
    "\n",
    "# Analogy 2: government is to policy as company is to ?\n",
    "result2 = glove_model.most_similar(positive=['policy', 'company'], negative=['government'], topn=3)\n",
    "print(\"Analogy 2 (government:policy :: company:?):\")\n",
    "print(result2)\n",
    "\n",
    "# Analogy 3: democrat is to liberal as republican is to ?\n",
    "result3 = glove_model.most_similar(positive=['liberal', 'republican'], negative=['democrat'], topn=3)\n",
    "print(\"Analogy 3 (democrat:liberal :: republican:?):\")\n",
    "print(result3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part C: BERT Sentence Embeddings\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) creates contextual embeddings where the same word can have different representations based on context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hf_xet\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-win_amd64.whl.metadata (5.0 kB)\n",
      "Downloading hf_xet-1.2.0-cp37-abi3-win_amd64.whl (2.9 MB)\n",
      "   ---------------------------------------- 0.0/2.9 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.8/2.9 MB 7.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.6/2.9 MB 8.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.9/2.9 MB 8.1 MB/s  0:00:00\n",
      "Installing collected packages: hf_xet\n",
      "Successfully installed hf_xet-1.2.0\n"
     ]
    }
   ],
   "source": [
    "! pip install hf_xet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT-based sentence transformer...\n",
      "Model loaded!\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load a pre-trained sentence transformer model\n",
    "print(\"Loading BERT-based sentence transformer...\")\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')  # Efficient model\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (4, 384)\n",
      "Each sentence is represented by a 384-dimensional vector\n"
     ]
    }
   ],
   "source": [
    "# Example: Get sentence embeddings\n",
    "sample_sentences = [\n",
    "    \"I love programming in Python.\",\n",
    "    \"Python is my favorite programming language.\",\n",
    "    \"The python snake is very long.\",\n",
    "    \"I enjoy coding and software development.\"\n",
    "]\n",
    "\n",
    "# Encode sentences\n",
    "embeddings = sentence_model.encode(sample_sentences)\n",
    "\n",
    "print(f\"Embedding shape: {embeddings.shape}\")\n",
    "print(f\"Each sentence is represented by a {embeddings.shape[1]}-dimensional vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence similarity matrix:\n",
      "\n",
      "Sentences:\n",
      "  0: I love programming in Python.\n",
      "  1: Python is my favorite programming language.\n",
      "  2: The python snake is very long.\n",
      "  3: I enjoy coding and software development.\n",
      "\n",
      "Similarity:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S0</th>\n",
       "      <th>S1</th>\n",
       "      <th>S2</th>\n",
       "      <th>S3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>S0</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.878</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S1</th>\n",
       "      <td>0.878</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.337</td>\n",
       "      <td>0.512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S2</th>\n",
       "      <td>0.370</td>\n",
       "      <td>0.337</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S3</th>\n",
       "      <td>0.621</td>\n",
       "      <td>0.512</td>\n",
       "      <td>0.058</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       S0     S1     S2     S3\n",
       "S0  1.000  0.878  0.370  0.621\n",
       "S1  0.878  1.000  0.337  0.512\n",
       "S2  0.370  0.337  1.000  0.058\n",
       "S3  0.621  0.512  0.058  1.000"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute sentence similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity = cosine_similarity(embeddings)\n",
    "\n",
    "print(\"Sentence similarity matrix:\")\n",
    "print(\"\\nSentences:\")\n",
    "for i, sent in enumerate(sample_sentences):\n",
    "    print(f\"  {i}: {sent}\")\n",
    "\n",
    "print(\"\\nSimilarity:\")\n",
    "sim_df = pd.DataFrame(similarity, \n",
    "                      index=[f\"S{i}\" for i in range(4)],\n",
    "                      columns=[f\"S{i}\" for i in range(4)])\n",
    "sim_df.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise C.1: Document Similarity with BERT\n",
    "\n",
    "Use BERT embeddings to find the most similar documents in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 30 documents\n"
     ]
    }
   ],
   "source": [
    "# Sample 30 documents (10 per category) for BERT embedding\n",
    "sampled_docs = []\n",
    "sampled_labels = []\n",
    "\n",
    "for category in my_categories:\n",
    "    cat_df = df_filtered[df_filtered['label_text'] == category].sample(n=10, random_state=42)\n",
    "    # Use first 500 characters of each document (BERT has length limits)\n",
    "    sampled_docs.extend(cat_df['text'].str[:500].tolist())\n",
    "    sampled_labels.extend([category] * 10)\n",
    "\n",
    "print(f\"Sampled {len(sampled_docs)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity matrix shape: (30, 30)\n"
     ]
    }
   ],
   "source": [
    "# Encode sampled documents with the sentence transformer and compute similarity matrix\n",
    "\n",
    "# Step 1: Encode all sampled documents (disable progress bar for notebook speed)\n",
    "doc_embeddings = sentence_model.encode(sampled_docs, show_progress_bar=False)\n",
    "\n",
    "# Step 2: Compute cosine similarity between document embeddings\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "bert_similarity = cosine_similarity(doc_embeddings)\n",
    "\n",
    "print(f\"Similarity matrix shape: {bert_similarity.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hf_xet in c:\\users\\gkmur\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (1.2.0)\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\gkmur\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from seaborn) (2.4.1)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\gkmur\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from seaborn) (2.3.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\gkmur\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from seaborn) (3.10.8)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\gkmur\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\gkmur\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\gkmur\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\gkmur\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\gkmur\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\gkmur\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (12.1.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\gkmur\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\gkmur\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\gkmur\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\gkmur\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\gkmur\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n"
     ]
    }
   ],
   "source": [
    "! pip install hf_xet\n",
    "! pip install seaborn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'backend_agg' from 'matplotlib.backends' (c:\\VIsual studio code\\New folder\\kernal practice\\nlp-course\\.venv\\Lib\\site-packages\\matplotlib\\backends\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mbert_similarity\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m():\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mbert_similarity not found. Run the BERT encoding cell first.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43mplt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfigure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfigsize\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m14\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m12\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _HAS_SEABORN:\n\u001b[32m     23\u001b[39m     sns.heatmap(\n\u001b[32m     24\u001b[39m         bert_similarity,\n\u001b[32m     25\u001b[39m         xticklabels=labels_short,\n\u001b[32m   (...)\u001b[39m\u001b[32m     28\u001b[39m         cbar_kws={\u001b[33m'\u001b[39m\u001b[33mshrink\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m0.6\u001b[39m},\n\u001b[32m     29\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\VIsual studio code\\New folder\\kernal practice\\nlp-course\\.venv\\Lib\\site-packages\\matplotlib\\pyplot.py:1041\u001b[39m, in \u001b[36mfigure\u001b[39m\u001b[34m(num, figsize, dpi, facecolor, edgecolor, frameon, FigureClass, clear, **kwargs)\u001b[39m\n\u001b[32m   1031\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(allnums) == max_open_warning >= \u001b[32m1\u001b[39m:\n\u001b[32m   1032\u001b[39m     _api.warn_external(\n\u001b[32m   1033\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMore than \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_open_warning\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m figures have been opened. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1034\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFigures created through the pyplot interface \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1038\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mConsider using `matplotlib.pyplot.close()`.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1039\u001b[39m         \u001b[38;5;167;01mRuntimeWarning\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1041\u001b[39m manager = \u001b[43mnew_figure_manager\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1042\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfigsize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfigsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdpi\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdpi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1043\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfacecolor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfacecolor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medgecolor\u001b[49m\u001b[43m=\u001b[49m\u001b[43medgecolor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframeon\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframeon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1044\u001b[39m \u001b[43m    \u001b[49m\u001b[43mFigureClass\u001b[49m\u001b[43m=\u001b[49m\u001b[43mFigureClass\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1045\u001b[39m fig = manager.canvas.figure\n\u001b[32m   1046\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fig_label:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\VIsual studio code\\New folder\\kernal practice\\nlp-course\\.venv\\Lib\\site-packages\\matplotlib\\pyplot.py:550\u001b[39m, in \u001b[36mnew_figure_manager\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    548\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnew_figure_manager\u001b[39m(*args, **kwargs):\n\u001b[32m    549\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Create a new figure manager instance.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m550\u001b[39m     \u001b[43m_warn_if_gui_out_of_main_thread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    551\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _get_backend_mod().new_figure_manager(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\VIsual studio code\\New folder\\kernal practice\\nlp-course\\.venv\\Lib\\site-packages\\matplotlib\\pyplot.py:527\u001b[39m, in \u001b[36m_warn_if_gui_out_of_main_thread\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    525\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_warn_if_gui_out_of_main_thread\u001b[39m() -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    526\u001b[39m     warn = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m527\u001b[39m     canvas_class = cast(\u001b[38;5;28mtype\u001b[39m[FigureCanvasBase], \u001b[43m_get_backend_mod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.FigureCanvas)\n\u001b[32m    528\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m canvas_class.required_interactive_framework:\n\u001b[32m    529\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(threading, \u001b[33m'\u001b[39m\u001b[33mget_native_id\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m    530\u001b[39m             \u001b[38;5;66;03m# This compares native thread ids because even if Python-level\u001b[39;00m\n\u001b[32m    531\u001b[39m             \u001b[38;5;66;03m# Thread objects match, the underlying OS thread (which is what\u001b[39;00m\n\u001b[32m    532\u001b[39m             \u001b[38;5;66;03m# really matters) may be different on Python implementations with\u001b[39;00m\n\u001b[32m    533\u001b[39m             \u001b[38;5;66;03m# green threads.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\VIsual studio code\\New folder\\kernal practice\\nlp-course\\.venv\\Lib\\site-packages\\matplotlib\\pyplot.py:368\u001b[39m, in \u001b[36m_get_backend_mod\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    359\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    360\u001b[39m \u001b[33;03mEnsure that a backend is selected and return it.\u001b[39;00m\n\u001b[32m    361\u001b[39m \n\u001b[32m    362\u001b[39m \u001b[33;03mThis is currently private, but may be made public in the future.\u001b[39;00m\n\u001b[32m    363\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    364\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _backend_mod \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    365\u001b[39m     \u001b[38;5;66;03m# Use rcParams._get(\"backend\") to avoid going through the fallback\u001b[39;00m\n\u001b[32m    366\u001b[39m     \u001b[38;5;66;03m# logic (which will (re)import pyplot and then call switch_backend if\u001b[39;00m\n\u001b[32m    367\u001b[39m     \u001b[38;5;66;03m# we need to resolve the auto sentinel)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m368\u001b[39m     \u001b[43mswitch_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrcParams\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbackend\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(\u001b[38;5;28mtype\u001b[39m[matplotlib.backend_bases._Backend], _backend_mod)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\VIsual studio code\\New folder\\kernal practice\\nlp-course\\.venv\\Lib\\site-packages\\matplotlib\\pyplot.py:424\u001b[39m, in \u001b[36mswitch_backend\u001b[39m\u001b[34m(newbackend)\u001b[39m\n\u001b[32m    421\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    422\u001b[39m old_backend = rcParams._get(\u001b[33m'\u001b[39m\u001b[33mbackend\u001b[39m\u001b[33m'\u001b[39m)  \u001b[38;5;66;03m# get without triggering backend resolution\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m424\u001b[39m module = \u001b[43mbackend_registry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_backend_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnewbackend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    425\u001b[39m canvas_class = module.FigureCanvas\n\u001b[32m    427\u001b[39m required_framework = canvas_class.required_interactive_framework\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\VIsual studio code\\New folder\\kernal practice\\nlp-course\\.venv\\Lib\\site-packages\\matplotlib\\backends\\registry.py:317\u001b[39m, in \u001b[36mload_backend_module\u001b[39m\u001b[34m(self, backend)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\uv\\python\\cpython-3.11.14-windows-x86_64-none\\Lib\\importlib\\__init__.py:126\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m    124\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    125\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1204\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1176\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1126\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:241\u001b[39m, in \u001b[36m_call_with_frames_removed\u001b[39m\u001b[34m(f, *args, **kwds)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1204\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1176\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1147\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:690\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:940\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:241\u001b[39m, in \u001b[36m_call_with_frames_removed\u001b[39m\u001b[34m(f, *args, **kwds)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\VIsual studio code\\New folder\\kernal practice\\nlp-course\\.venv\\Lib\\site-packages\\matplotlib_inline\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m backend_inline, config  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m      3\u001b[39m __version__ = \u001b[33m\"\u001b[39m\u001b[33m0.2.1\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# we can't ''.join(...) otherwise finding the version number at build time requires\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# import which introduces IPython and matplotlib at build time, and thus circular\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# dependencies.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\VIsual studio code\\New folder\\kernal practice\\nlp-course\\.venv\\Lib\\site-packages\\matplotlib_inline\\backend_inline.py:13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m colors\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_pylab_helpers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Gcf\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackends\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m backend_agg\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackends\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend_agg\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FigureCanvasAgg\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfigure\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Figure\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'backend_agg' from 'matplotlib.backends' (c:\\VIsual studio code\\New folder\\kernal practice\\nlp-course\\.venv\\Lib\\site-packages\\matplotlib\\backends\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# Visualize BERT similarity matrix with graceful fallback if seaborn not installed\n",
    "# Use a non-GUI backend to avoid interactive backend errors in headless environments\n",
    "import matplotlib\n",
    "try:\n",
    "    matplotlib.use('Agg')\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    import seaborn as sns\n",
    "    _HAS_SEABORN = True\n",
    "except Exception:\n",
    "    _HAS_SEABORN = False\n",
    "\n",
    "# Create labels (shortened for plotting)\n",
    "labels_short = [f\"{l[:6]}_{i%10}\" for i, l in enumerate(sampled_labels)]\n",
    "\n",
    "# Ensure bert_similarity is available\n",
    "if 'bert_similarity' not in globals():\n",
    "    raise RuntimeError('bert_similarity not found. Run the BERT encoding cell first.')\n",
    "\n",
    "plt.figure(figsize=(14, 12))\n",
    "if _HAS_SEABORN:\n",
    "    sns.heatmap(\n",
    "        bert_similarity,\n",
    "        xticklabels=labels_short,\n",
    "        yticklabels=labels_short,\n",
    "        cmap='YlOrRd',\n",
    "        cbar_kws={'shrink': 0.6},\n",
    "    )\n",
    "else:\n",
    "    # Fallback: use matplotlib's imshow\n",
    "    plt.imshow(bert_similarity, cmap='YlOrRd', aspect='auto')\n",
    "    plt.xticks(ticks=range(len(labels_short)), labels=labels_short, rotation=90, fontsize=8)\n",
    "    plt.yticks(ticks=range(len(labels_short)), labels=labels_short, fontsize=8)\n",
    "    plt.colorbar(shrink=0.6)\n",
    "\n",
    "plt.title('Document Similarity (BERT Embeddings)')\n",
    "plt.tight_layout()\n",
    "out_path = 'bert_similarity_heatmap.png'\n",
    "plt.savefig(out_path, dpi=150, bbox_inches='tight')\n",
    "# Display the saved image inline in notebooks that support it\n",
    "try:\n",
    "    from IPython.display import Image, display\n",
    "    display(Image(filename=out_path))\n",
    "except Exception:\n",
    "    # If display fails, at least inform the user where the file was saved\n",
    "    print(f'Heatmap saved to: {out_path}')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Written Question C.1 (Personal Interpretation)\n",
    "\n",
    "Compare the BERT similarity heatmap with the TF-IDF similarity heatmap from Part 1:\n",
    "\n",
    "1. **Do documents cluster better by category with BERT or TF-IDF?**\n",
    "2. **Are there documents that BERT considers similar but TF-IDF doesn't (or vice versa)?** Why might this happen?\n",
    "3. **Which method would you use for a document classification task?** Explain your reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER:**\n",
    "\n",
    "1. Better clustering with: ...\n",
    "\n",
    "2. Differences between methods: ...\n",
    "\n",
    "3. Preferred method for classification: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise C.2: Semantic Search with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'gun control policy'\n",
      "\n",
      "Top 5 most similar documents:\n",
      "\n",
      "  Score: 0.5296\n",
      "  Category: talk.politics.guns\n",
      "  Text: \n",
      "       Two questions:  When was this, and do you have the relevant\n",
      "numbers.  (Please note, this is *not* in any way an indication I don't\n",
      "believe you or that you're not correct, but when the drop occ...\n",
      "\n",
      "  Score: 0.4240\n",
      "  Category: talk.politics.guns\n",
      "  Text: Excuse me but I do know what I safety is supposed to do.  It's basic purpose -\n",
      "not to let the gun fire until you're ready.  Christ, I've known that since I\n",
      "had my first Crosman air gun.  You don't kno...\n",
      "\n",
      "  Score: 0.4214\n",
      "  Category: talk.politics.guns\n",
      "  Text: / iftccu:talk.politics.guns / kendall@lds.loral.com (Colin Kendall 6842) /  9:23 am  Apr 13, 1993 /\n",
      "\n",
      "\n",
      "Follow more than one months posting.  As more than one reader has noted, \n",
      "there IS some reporting ...\n",
      "\n",
      "  Score: 0.4054\n",
      "  Category: talk.politics.guns\n",
      "  Text: \n",
      ": >: Rate := per capita rate.  The UK is more dangerous.\n",
      ": >: Though you may be less likely to be killed by a handgun, the average\n",
      ": >: individual citizen in the UK is twice as likely to be killed\n",
      ": ...\n",
      "\n",
      "  Score: 0.3648\n",
      "  Category: talk.politics.guns\n",
      "  Text: \n",
      "What's \"loosing?\"  \n",
      "\n",
      "\n",
      "I vote.  I don't consider RKBA an abomination. \n",
      "\n",
      "\n",
      "I'm sure Sarah Brady would be delighted to hear your ranting and\n",
      "raving.  However, Clinton has not publically stated that he wo...\n"
     ]
    }
   ],
   "source": [
    "# Simple semantic search using BERT embeddings\n",
    "# Given a query, find the most similar documents (top_k)\n",
    "\n",
    "def semantic_search(query, documents, model, top_k=5):\n",
    "    \"\"\"Simple semantic search using precomputed `doc_embeddings`.\"\"\"\n",
    "    if not isinstance(query, str) or len(documents) == 0:\n",
    "        return []\n",
    "\n",
    "    # Encode the query\n",
    "    q_emb = model.encode([query], show_progress_bar=False)[0]\n",
    "\n",
    "    # Compute cosine similarity with document embeddings (doc_embeddings expected to exist)\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    sims = cosine_similarity([q_emb], doc_embeddings)[0]\n",
    "\n",
    "    # Get top_k indices sorted by similarity (descending)\n",
    "    top_idx = sims.argsort()[::-1][:top_k]\n",
    "    results = [(int(i), float(sims[i])) for i in top_idx]\n",
    "    return results\n",
    "\n",
    "# Test the search function with an example query related to one of the categories\n",
    "my_query = \"gun control policy\"  # Example query (change as needed)\n",
    "\n",
    "results = semantic_search(my_query, sampled_docs, sentence_model, top_k=5)\n",
    "\n",
    "print(f\"Query: '{my_query}'\")\n",
    "print(\"\\nTop 5 most similar documents:\")\n",
    "for idx, score in results:\n",
    "    print(f\"\\n  Score: {score:.4f}\")\n",
    "    print(f\"  Category: {sampled_labels[idx]}\")\n",
    "    print(f\"  Text: {sampled_docs[idx][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Written Question C.2 (Personal Interpretation)\n",
    "\n",
    "Evaluate your semantic search results:\n",
    "\n",
    "1. **Are the results relevant to your query?** Explain.\n",
    "2. **Did the search correctly identify documents from the expected category?**\n",
    "3. **Try a query that could match multiple categories. What happens?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER:**\n",
    "\n",
    "1. Relevance: ...\n",
    "\n",
    "2. Category accuracy: ...\n",
    "\n",
    "3. Ambiguous query test: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part D: Embedding Visualization with t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Reduce BERT embeddings to 2D for visualization\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=10)\n",
    "embeddings_2d = tsne.fit_transform(doc_embeddings)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "colors = {'___': 'red', '___': 'blue', '___': 'green'}  # Update with your categories\n",
    "# Actually use your categories:\n",
    "color_map = plt.cm.Set1\n",
    "\n",
    "for i, category in enumerate(my_categories):\n",
    "    mask = [l == category for l in sampled_labels]\n",
    "    plt.scatter(\n",
    "        embeddings_2d[mask, 0],\n",
    "        embeddings_2d[mask, 1],\n",
    "        label=category,\n",
    "        alpha=0.7,\n",
    "        s=100\n",
    "    )\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Document Embeddings (BERT + t-SNE)')\n",
    "plt.xlabel('t-SNE dimension 1')\n",
    "plt.ylabel('t-SNE dimension 2')\n",
    "plt.tight_layout()\n",
    "plt.savefig('tsne_document_embeddings.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Written Question D.1 (Personal Interpretation)\n",
    "\n",
    "Look at your t-SNE visualization:\n",
    "\n",
    "1. **Do the categories form distinct clusters?**\n",
    "2. **Are there any documents that appear in the \"wrong\" cluster?** What might explain this?\n",
    "3. **Based on the visualization, which two categories are most similar?** Does this match your expectations from Part 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER:**\n",
    "\n",
    "1. Cluster quality: ...\n",
    "\n",
    "2. Misplaced documents: ...\n",
    "\n",
    "3. Most similar categories: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part E: Final Comparison and Reflection (10 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Written Question (Comprehensive Reflection)\n",
    "\n",
    "Based on everything you've learned in this lab:\n",
    "\n",
    "1. **Create a comparison table** summarizing the strengths and weaknesses of each text representation method:\n",
    "\n",
    "| Method | Strengths | Weaknesses | Best Use Case |\n",
    "|--------|-----------|------------|---------------|\n",
    "| BoW | ... | ... | ... |\n",
    "| TF-IDF | ... | ... | ... |\n",
    "| Word2Vec | ... | ... | ... |\n",
    "| GloVe | ... | ... | ... |\n",
    "| BERT | ... | ... | ... |\n",
    "\n",
    "2. **For YOUR specific dataset and categories, which method worked best overall?** Support your answer with specific evidence from your experiments.\n",
    "\n",
    "3. **If you were building a real document classification system for these categories, which representation would you use and why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER:**\n",
    "\n",
    "### 1. Comparison Table\n",
    "\n",
    "| Method | Strengths | Weaknesses | Best Use Case |\n",
    "|--------|-----------|------------|---------------|\n",
    "| BoW | ... | ... | ... |\n",
    "| TF-IDF | ... | ... | ... |\n",
    "| Word2Vec | ... | ... | ... |\n",
    "| GloVe | ... | ... | ... |\n",
    "| BERT | ... | ... | ... |\n",
    "\n",
    "### 2. Best Method for My Dataset\n",
    "\n",
    "*[Write at least 4-5 sentences with specific evidence]*\n",
    "\n",
    "...\n",
    "\n",
    "### 3. My Recommendation for a Real System\n",
    "\n",
    "*[Write your recommendation and justification]*\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary - Lab 3\n",
    "\n",
    "In this lab, you learned:\n",
    "\n",
    "**Part 1:**\n",
    "- Text visualization with bar charts and word clouds\n",
    "- Bag of Words and TF-IDF representations\n",
    "- N-grams and next-word prediction\n",
    "- Document correlation analysis\n",
    "\n",
    "**Part 2:**\n",
    "- Training Word2Vec models (CBOW vs Skip-gram)\n",
    "- Using pre-trained GloVe embeddings\n",
    "- BERT for sentence embeddings\n",
    "- Semantic search with embeddings\n",
    "- Embedding visualization with t-SNE\n",
    "\n",
    "---\n",
    "\n",
    "## Final Submission Checklist\n",
    "\n",
    "- [ ] All code exercises completed in Part 1 and Part 2\n",
    "- [ ] **All written questions answered with YOUR personal interpretation**\n",
    "- [ ] All visualizations saved (PNG files)\n",
    "- [ ] Both notebooks saved\n",
    "- [ ] Pushed to Git repository\n",
    "- [ ] **Repository link sent to: yoroba93@gmail.com**\n",
    "\n",
    "### Reminder: Oral Defense\n",
    "\n",
    "Be prepared to:\n",
    "- Explain your choice of categories and why\n",
    "- Discuss your written interpretations\n",
    "- Answer questions about the methods you used\n",
    "- Explain any surprising results you found"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
