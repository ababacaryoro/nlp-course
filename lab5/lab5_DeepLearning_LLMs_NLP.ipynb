{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Deep Learning & LLMs for NLP\n",
    "\n",
    "**Course:** Natural Language Processing\n",
    "\n",
    "\n",
    "**Objectives:**\n",
    "- Understand RNN, LSTM, GRU architectures for sequence modeling\n",
    "- Use pre-trained Transformers for NER\n",
    "- Interact with LLMs via API for text generation\n",
    "\n",
    "---\n",
    "\n",
    "## Instructions\n",
    "\n",
    "1. Complete all exercises marked with `# YOUR CODE HERE`\n",
    "2. **Answer all written questions** in the designated markdown cells\n",
    "3. Save your completed notebook\n",
    "4. **Push to your Git repository and send the link to: yoroba93@gmail.com**\n",
    "\n",
    "---\n",
    "\n",
    "## Lab Structure\n",
    "\n",
    "| Part | Model | Task |\n",
    "|------|-------|------|\n",
    "| A | RNN | Character-level Language Model |\n",
    "| B | LSTM | Sentiment Analysis |\n",
    "| C | GRU | News Classification |\n",
    "| D | Transformer | Named Entity Recognition | \n",
    "| E | LLM (Mistral) | Text Generation & QA |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries (uncomment if needed)\n",
    "# !pip install torch transformers datasets requests numpy pandas matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART A: RNN - Character-Level Language Model (10 min)\n",
    "\n",
    "**Use Case:** Predict the next character for autocomplete.\n",
    "\n",
    "**Dataset:** Tiny Shakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Tiny Shakespeare dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "shakespeare = load_dataset(\"tiny_shakespeare\", split=\"train\")\n",
    "text = shakespeare['text'][0][:10000]  # Use first 10K chars for speed\n",
    "\n",
    "print(f\"Text length: {len(text)} characters\")\n",
    "print(f\"Sample: {text[:200]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create character vocabulary\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Characters: {''.join(chars[:30])}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare sequences\n",
    "seq_length = 30\n",
    "X, y = [], []\n",
    "\n",
    "for i in range(len(text) - seq_length):\n",
    "    X.append([char_to_idx[c] for c in text[i:i+seq_length]])\n",
    "    y.append(char_to_idx[text[i+seq_length]])\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.long)\n",
    "y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "print(f\"Sequences: {X.shape[0]}, Sequence length: {seq_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple RNN model\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out[:, -1, :])  # Last timestep\n",
    "        return out\n",
    "\n",
    "# Create model\n",
    "rnn_model = CharRNN(vocab_size, embed_dim=32, hidden_dim=64).to(device)\n",
    "print(f\"RNN Parameters: {sum(p.numel() for p in rnn_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise A.1: Train the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete the training loop\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "epochs = 5\n",
    "learning_rate = ___  # YOUR CHOICE: 0.001-0.01\n",
    "\n",
    "# DataLoader\n",
    "dataset = TensorDataset(X[:5000], y[:5000])  # Use subset for speed\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rnn_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for batch_X, batch_y in loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        # 1. Zero gradients\n",
    "        # 2. Forward pass\n",
    "        # 3. Compute loss\n",
    "        # 4. Backward pass\n",
    "        # 5. Update weights\n",
    "        \n",
    "        pass  # Remove and add your code\n",
    "        \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    losses.append(avg_loss)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text\n",
    "def generate_text(model, start_str, length=100):\n",
    "    model.eval()\n",
    "    chars_generated = list(start_str)\n",
    "    input_seq = [char_to_idx.get(c, 0) for c in start_str[-seq_length:]]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(length):\n",
    "            x = torch.tensor([input_seq[-seq_length:]]).to(device)\n",
    "            output = model(x)\n",
    "            pred_idx = torch.argmax(output, dim=1).item()\n",
    "            chars_generated.append(idx_to_char[pred_idx])\n",
    "            input_seq.append(pred_idx)\n",
    "    \n",
    "    return ''.join(chars_generated)\n",
    "\n",
    "# Test generation\n",
    "print(\"Generated text:\")\n",
    "print(generate_text(rnn_model, \"To be or not\", length=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART B: LSTM - Sentiment Analysis \n",
    "\n",
    "**Use Case:** Classify movie review sentiment.\n",
    "\n",
    "**Dataset:** IMDB Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IMDB dataset\n",
    "imdb = load_dataset(\"imdb\")\n",
    "\n",
    "# Small sample for quick training\n",
    "train_texts = imdb['train']['text'][:1000]\n",
    "train_labels = imdb['train']['label'][:1000]\n",
    "test_texts = imdb['test']['text'][:200]\n",
    "test_labels = imdb['test']['label'][:200]\n",
    "\n",
    "print(f\"Train: {len(train_texts)}, Test: {len(test_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple tokenization and vocabulary\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())[:100]  # Max 100 tokens\n",
    "\n",
    "# Build vocabulary from training data\n",
    "all_tokens = [tok for text in train_texts for tok in tokenize(text)]\n",
    "vocab = {word: idx+2 for idx, (word, _) in enumerate(Counter(all_tokens).most_common(5000))}\n",
    "vocab['<PAD>'] = 0\n",
    "vocab['<UNK>'] = 1\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode texts\n",
    "def encode_text(text, max_len=100):\n",
    "    tokens = tokenize(text)\n",
    "    encoded = [vocab.get(t, 1) for t in tokens]  # 1 = UNK\n",
    "    padded = encoded[:max_len] + [0] * (max_len - len(encoded))\n",
    "    return padded[:max_len]\n",
    "\n",
    "X_train = torch.tensor([encode_text(t) for t in train_texts])\n",
    "y_train = torch.tensor(train_labels)\n",
    "X_test = torch.tensor([encode_text(t) for t in test_texts])\n",
    "y_test = torch.tensor(test_labels)\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise B.1: Complete the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete the LSTM classifier\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        \n",
    "        # YOUR CODE HERE: Define LSTM layer\n",
    "        # Hint: nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.lstm = ___\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # YOUR CODE HERE: Pass through LSTM and get final hidden state\n",
    "        # Hint: lstm_out, (hidden, cell) = self.lstm(x)\n",
    "        \n",
    "        out = self.fc(___)  # Use last hidden state\n",
    "        return out\n",
    "\n",
    "# Create model\n",
    "lstm_model = LSTMClassifier(\n",
    "    vocab_size=len(vocab),\n",
    "    embed_dim=64,\n",
    "    hidden_dim=64,\n",
    "    num_classes=2\n",
    ").to(device)\n",
    "\n",
    "print(f\"LSTM Parameters: {sum(p.numel() for p in lstm_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick training\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)\n",
    "\n",
    "# Train for 3 epochs\n",
    "for epoch in range(3):\n",
    "    lstm_model.train()\n",
    "    total_loss = 0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = lstm_model(batch_X)\n",
    "        loss = criterion(output, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Evaluate\n",
    "lstm_model.eval()\n",
    "with torch.no_grad():\n",
    "    test_output = lstm_model(X_test.to(device))\n",
    "    preds = torch.argmax(test_output, dim=1).cpu()\n",
    "    acc = (preds == y_test).float().mean()\n",
    "    print(f\"\\nTest Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART C: GRU - News Classification\n",
    "\n",
    "**Use Case:** Classify news articles by topic.\n",
    "\n",
    "**Why GRU?** Fewer parameters than LSTM, faster training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load AG News\n",
    "ag_news = load_dataset(\"ag_news\")\n",
    "ag_train = ag_news['train'].shuffle(seed=42).select(range(2000))\n",
    "ag_test = ag_news['test'].shuffle(seed=42).select(range(500))\n",
    "\n",
    "ag_labels = {0: 'World', 1: 'Sports', 2: 'Business', 3: 'Sci/Tech'}\n",
    "print(f\"Classes: {list(ag_labels.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise C.1: Build GRU Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a GRU classifier (similar to LSTM but using nn.GRU)\n",
    "\n",
    "class GRUClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        \n",
    "        # YOUR CODE HERE: Define GRU layer\n",
    "        self.gru = ___\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # YOUR CODE HERE: GRU forward pass\n",
    "        # Note: GRU returns (output, hidden) - no cell state unlike LSTM\n",
    "        \n",
    "        out = self.fc(___)  # Use last hidden state\n",
    "        return out\n",
    "\n",
    "# Build vocabulary and encode (reuse tokenize function)\n",
    "ag_tokens = [tok for item in ag_train for tok in tokenize(item['text'])]\n",
    "ag_vocab = {word: idx+2 for idx, (word, _) in enumerate(Counter(ag_tokens).most_common(5000))}\n",
    "ag_vocab['<PAD>'] = 0\n",
    "ag_vocab['<UNK>'] = 1\n",
    "\n",
    "def encode_ag(text, vocab, max_len=50):\n",
    "    tokens = tokenize(text)\n",
    "    encoded = [vocab.get(t, 1) for t in tokens]\n",
    "    return (encoded[:max_len] + [0] * max_len)[:max_len]\n",
    "\n",
    "X_ag_train = torch.tensor([encode_ag(item['text'], ag_vocab) for item in ag_train])\n",
    "y_ag_train = torch.tensor([item['label'] for item in ag_train])\n",
    "X_ag_test = torch.tensor([encode_ag(item['text'], ag_vocab) for item in ag_test])\n",
    "y_ag_test = torch.tensor([item['label'] for item in ag_test])\n",
    "\n",
    "print(f\"AG News - Train: {X_ag_train.shape}, Test: {X_ag_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train GRU model\n",
    "gru_model = GRUClassifier(\n",
    "    vocab_size=len(ag_vocab),\n",
    "    embed_dim=64,\n",
    "    hidden_dim=64,\n",
    "    num_classes=4\n",
    ").to(device)\n",
    "\n",
    "print(f\"GRU Parameters: {sum(p.numel() for p in gru_model.parameters()):,}\")\n",
    "print(f\"(Compare to LSTM: GRU has fewer parameters!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART D: Transformer - Named Entity Recognition\n",
    "\n",
    "**Use Case:** Extract entities from text.\n",
    "\n",
    "**Dataset:** CoNLL-2003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pre-trained NER model from Hugging Face\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load NER pipeline (uses BERT-based model)\n",
    "print(\"Loading NER model...\")\n",
    "ner_pipeline = pipeline(\"ner\", model=\"dslim/bert-base-NER\", aggregation_strategy=\"simple\")\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example NER\n",
    "text = \"Apple Inc. was founded by Steve Jobs in Cupertino, California. Tim Cook is the current CEO.\"\n",
    "\n",
    "entities = ner_pipeline(text)\n",
    "print(f\"Text: {text}\\n\")\n",
    "print(\"Entities found:\")\n",
    "for ent in entities:\n",
    "    print(f\"  {ent['word']:20} -> {ent['entity_group']:10} (score: {ent['score']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise D.1: NER on Your Own Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write 3 sentences and extract entities\n",
    "# Include: people, organizations, locations\n",
    "\n",
    "my_sentences = [\n",
    "    \"___\",  # YOUR SENTENCE 1\n",
    "    \"___\",  # YOUR SENTENCE 2\n",
    "    \"___\",  # YOUR SENTENCE 3\n",
    "]\n",
    "\n",
    "for sent in my_sentences:\n",
    "    if sent != \"___\":\n",
    "        print(f\"\\nText: {sent}\")\n",
    "        entities = ner_pipeline(sent)\n",
    "        for ent in entities:\n",
    "            print(f\"  {ent['word']:20} -> {ent['entity_group']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART E: LLM - Text Generation with Mistral API\n",
    "\n",
    "**Use Case:** Conversational AI and Question Answering.\n",
    "\n",
    "**Setup:** Get a free API key from https://console.mistral.ai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your Mistral API key\n",
    "# Get free key at: https://console.mistral.ai/\n",
    "\n",
    "MISTRAL_API_KEY = \"___\"  # YOUR API KEY HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def query_mistral(prompt, max_tokens=150):\n",
    "    \"\"\"Query Mistral API.\"\"\"\n",
    "    url = \"https://api.mistral.ai/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {MISTRAL_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": \"mistral-small-latest\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"max_tokens\": max_tokens\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()['choices'][0]['message']['content']\n",
    "    else:\n",
    "        return f\"Error: {response.status_code} - {response.text}\"\n",
    "\n",
    "# Test (only if API key is set)\n",
    "if MISTRAL_API_KEY != \"___\":\n",
    "    response = query_mistral(\"What is NLP in one sentence?\")\n",
    "    print(f\"Mistral: {response}\")\n",
    "else:\n",
    "    print(\"Please set your MISTRAL_API_KEY above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise E.1: Compare LLM with Traditional Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Ask Mistral to perform sentiment analysis and compare with our LSTM\n",
    "\n",
    "test_review = \"This movie was absolutely terrible. The acting was bad and the plot made no sense.\"\n",
    "\n",
    "# LLM approach\n",
    "if MISTRAL_API_KEY != \"___\":\n",
    "    prompt = f\"\"\"Classify the sentiment of this review as 'positive' or 'negative'. \n",
    "Just respond with one word.\n",
    "\n",
    "Review: {test_review}\n",
    "\n",
    "Sentiment:\"\"\"\n",
    "    \n",
    "    llm_result = query_mistral(prompt, max_tokens=10)\n",
    "    print(f\"LLM Sentiment: {llm_result}\")\n",
    "\n",
    "# Traditional LSTM approach (if model trained)\n",
    "try:\n",
    "    encoded = torch.tensor([encode_text(test_review)]).to(device)\n",
    "    lstm_model.eval()\n",
    "    with torch.no_grad():\n",
    "        lstm_pred = torch.argmax(lstm_model(encoded)).item()\n",
    "    print(f\"LSTM Sentiment: {'positive' if lstm_pred == 1 else 'negative'}\")\n",
    "except:\n",
    "    print(\"LSTM model not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use LLM for summarization (something traditional models can't easily do)\n",
    "\n",
    "long_text = \"\"\"\n",
    "Natural language processing (NLP) is a subfield of linguistics, computer science, \n",
    "and artificial intelligence concerned with the interactions between computers and \n",
    "human language, in particular how to program computers to process and analyze large \n",
    "amounts of natural language data. The result is a computer capable of understanding \n",
    "the contents of documents, including the contextual nuances of the language within them.\n",
    "\"\"\"\n",
    "\n",
    "if MISTRAL_API_KEY != \"___\":\n",
    "    summary_prompt = f\"Summarize this in one sentence:\\n\\n{long_text}\"\n",
    "    summary = query_mistral(summary_prompt, max_tokens=50)\n",
    "    print(f\"Summary: {summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Final Written Questions (Personal Interpretation)\n",
    "\n",
    "Answer these questions based on YOUR experiments:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Model Architecture Comparison\n",
    "\n",
    "Compare the parameter counts you observed:\n",
    "- RNN: ___ parameters\n",
    "- LSTM: ___ parameters  \n",
    "- GRU: ___ parameters\n",
    "\n",
    "**Why does LSTM have more parameters than GRU?** (Hint: think about gates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER:**\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: RNN vs LSTM for Long Sequences\n",
    "\n",
    "**Why would LSTM perform better than vanilla RNN for sentiment analysis on long reviews?** Explain the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER:**\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Traditional Models vs LLMs\n",
    "\n",
    "Based on your experiments:\n",
    "1. **What can LLMs do that LSTM/GRU cannot?**\n",
    "2. **What are the disadvantages of using LLM APIs?** (Think: cost, latency, privacy)\n",
    "3. **When would you choose a traditional model over an LLM?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER:**\n",
    "\n",
    "1. LLM advantages: ...\n",
    "\n",
    "2. LLM disadvantages: ...\n",
    "\n",
    "3. When to use traditional models: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Model | Strength | Weakness | Best For |\n",
    "|-------|----------|----------|----------|\n",
    "| RNN | Simple, fast | Vanishing gradients | Short sequences |\n",
    "| LSTM | Long-term memory | More parameters | Long text classification |\n",
    "| GRU | Efficient, fast | Less expressive | When speed matters |\n",
    "| Transformer | Parallel, contextual | Expensive | NER, QA, many tasks |\n",
    "| LLM | Versatile, zero-shot | API cost, latency | Complex reasoning |\n",
    "\n",
    "---\n",
    "\n",
    "## Submission\n",
    "\n",
    "- [ ] All code exercises completed\n",
    "- [ ] All written questions answered\n",
    "- [ ] Mistral API tested (or explained why not)\n",
    "- [ ] **Push to Git and send link to: yoroba93@gmail.com**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
