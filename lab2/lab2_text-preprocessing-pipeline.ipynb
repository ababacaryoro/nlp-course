{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab2: Text Preprocessing Pipeline\n",
    "\n",
    "**Duration:** 1 hour\n",
    "\n",
    "**Objectives:**\n",
    "- Understand and implement tokenization techniques\n",
    "- Apply stemming and lemmatization for text normalization\n",
    "- Remove stop words and special characters\n",
    "- Build a complete text preprocessing pipeline\n",
    "\n",
    "---\n",
    "\n",
    "## Instructions\n",
    "\n",
    "1. Complete all the exercises marked with `# TODO`\n",
    "2. Run each cell to verify your answers\n",
    "3. Save your completed notebook\n",
    "4. **Push your work to a Git repository and send the link to: yoroba93@gmail.com**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Install and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries (uncomment if needed)\n",
    "# !pip install nltk spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "# Download and load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Tokenization\n",
    "\n",
    "Tokenization is the process of breaking text into smaller units (tokens), typically words or sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Basic Tokenization with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple tokenization using split()\n",
    "text = \"Natural Language Processing is fascinating!\"\n",
    "\n",
    "# Basic split on whitespace\n",
    "tokens_basic = text.split()\n",
    "print(\"Basic split():\", tokens_basic)\n",
    "\n",
    "# Problem: punctuation is attached to words!\n",
    "print(\"Last token:\", tokens_basic[-1])  # 'fascinating!' includes the !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Tokenization with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "text = \"Hello! How are you doing today? I'm learning NLP. It's really interesting.\"\n",
    "\n",
    "# Word tokenization\n",
    "word_tokens = word_tokenize(text)\n",
    "print(\"Word tokens:\", word_tokens)\n",
    "\n",
    "# Sentence tokenization\n",
    "sent_tokens = sent_tokenize(text)\n",
    "print(\"\\nSentence tokens:\")\n",
    "for i, sent in enumerate(sent_tokens):\n",
    "    print(f\"  {i+1}. {sent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK handles contractions and punctuation better\n",
    "text = \"I can't believe it's not butter! Don't you think so?\"\n",
    "\n",
    "print(\"Basic split:\", text.split())\n",
    "print(\"NLTK tokenize:\", word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Exercise 1.1\n",
    "# Tokenize the following text into words using NLTK\n",
    "# Count how many tokens are produced\n",
    "\n",
    "text = \"Dr. Smith's patients can't understand why they're feeling unwell. Is it the flu?\"\n",
    "\n",
    "tokens = # YOUR CODE HERE\n",
    "num_tokens = # YOUR CODE HERE\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Number of tokens:\", num_tokens)\n",
    "\n",
    "assert num_tokens == 18, f\"Expected 18 tokens, got {num_tokens}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Tokenization with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCy tokenization\n",
    "text = \"Apple is looking at buying U.K. startup for $1 billion.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "# Get tokens\n",
    "tokens = [token.text for token in doc]\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# spaCy provides additional information\n",
    "print(\"\\nDetailed token info:\")\n",
    "for token in doc:\n",
    "    print(f\"  {token.text:12} | POS: {token.pos_:6} | Is Stop: {token.is_stop}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Exercise 1.2\n",
    "# Use spaCy to tokenize the text and extract only:\n",
    "# 1. Tokens that are NOT punctuation\n",
    "# 2. Tokens that are NOT spaces\n",
    "# Hint: use token.is_punct and token.is_space\n",
    "\n",
    "text = \"The quick, brown fox jumps over the lazy dog! Isn't it amazing?\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "clean_tokens = # YOUR CODE HERE (list comprehension)\n",
    "\n",
    "print(\"Clean tokens:\", clean_tokens)\n",
    "assert len(clean_tokens) == 13, f\"Expected 13 tokens, got {len(clean_tokens)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Different Tokenization Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer, TweetTokenizer\n",
    "\n",
    "# RegexpTokenizer - tokenize using a custom pattern\n",
    "# \\w+ matches word characters only (removes punctuation)\n",
    "regexp_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "text = \"Hello! How's it going? #NLP @user123\"\n",
    "print(\"Regexp tokens:\", regexp_tokenizer.tokenize(text))\n",
    "\n",
    "# TweetTokenizer - designed for social media text\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "print(\"Tweet tokens:\", tweet_tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Exercise 1.3\n",
    "# Create a RegexpTokenizer that extracts only alphabetic words (no numbers, no punctuation)\n",
    "# Pattern hint: [a-zA-Z]+ matches one or more letters\n",
    "\n",
    "text = \"I have 3 cats and 2 dogs! Their names are Max123 and Bella.\"\n",
    "\n",
    "alpha_tokenizer = # YOUR CODE HERE\n",
    "alpha_tokens = # YOUR CODE HERE\n",
    "\n",
    "print(\"Alphabetic tokens:\", alpha_tokens)\n",
    "assert alpha_tokens == ['I', 'have', 'cats', 'and', 'dogs', 'Their', 'names', 'are', 'Max', 'and', 'Bella'], \"Check your pattern!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Stemming and Lemmatization\n",
    "\n",
    "Both techniques reduce words to their base form, but they work differently:\n",
    "- **Stemming**: Chops off word endings using rules (faster, cruder)\n",
    "- **Lemmatization**: Uses vocabulary and morphological analysis (slower, accurate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Stemming with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "\n",
    "# Porter Stemmer (most common)\n",
    "porter = PorterStemmer()\n",
    "\n",
    "words = [\"running\", \"runs\", \"ran\", \"runner\", \"easily\", \"fairly\"]\n",
    "\n",
    "print(\"Porter Stemmer:\")\n",
    "for word in words:\n",
    "    print(f\"  {word:12} -> {porter.stem(word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snowball Stemmer (supports multiple languages)\n",
    "snowball = SnowballStemmer('english')\n",
    "\n",
    "print(\"Snowball Stemmer:\")\n",
    "for word in words:\n",
    "    print(f\"  {word:12} -> {snowball.stem(word)}\")\n",
    "\n",
    "# Available languages\n",
    "print(\"\\nAvailable languages:\", SnowballStemmer.languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming limitations - can produce non-words\n",
    "problem_words = [\"studies\", \"studying\", \"university\", \"universe\", \"beautiful\", \"beauty\"]\n",
    "\n",
    "print(\"Stemming can produce non-words:\")\n",
    "for word in problem_words:\n",
    "    print(f\"  {word:12} -> {porter.stem(word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Exercise 2.1\n",
    "# Apply Porter stemming to all words in the sentence\n",
    "# Return the stemmed tokens as a list\n",
    "\n",
    "sentence = \"The cats are running and jumping over the sleeping dogs\"\n",
    "\n",
    "# Step 1: Tokenize (use word_tokenize)\n",
    "tokens = # YOUR CODE HERE\n",
    "\n",
    "# Step 2: Apply stemming to each token\n",
    "stemmed_tokens = # YOUR CODE HERE (list comprehension)\n",
    "\n",
    "print(\"Original:\", tokens)\n",
    "print(\"Stemmed:\", stemmed_tokens)\n",
    "\n",
    "assert stemmed_tokens == ['the', 'cat', 'are', 'run', 'and', 'jump', 'over', 'the', 'sleep', 'dog'], \"Check your stemming!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Lemmatization with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = [\"running\", \"runs\", \"ran\", \"better\", \"studies\", \"feet\", \"geese\"]\n",
    "\n",
    "print(\"Lemmatization (default - assumes nouns):\")\n",
    "for word in words:\n",
    "    print(f\"  {word:12} -> {lemmatizer.lemmatize(word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization works better with POS tags\n",
    "# pos: 'n' = noun, 'v' = verb, 'a' = adjective, 'r' = adverb\n",
    "\n",
    "print(\"Lemmatization with POS tags:\")\n",
    "print(f\"  running (verb):     {lemmatizer.lemmatize('running', pos='v')}\")\n",
    "print(f\"  running (noun):     {lemmatizer.lemmatize('running', pos='n')}\")\n",
    "print(f\"  better (adjective): {lemmatizer.lemmatize('better', pos='a')}\")\n",
    "print(f\"  studies (verb):     {lemmatizer.lemmatize('studies', pos='v')}\")\n",
    "print(f\"  studies (noun):     {lemmatizer.lemmatize('studies', pos='n')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Exercise 2.2\n",
    "# Lemmatize the following words using the correct POS tag\n",
    "# Fill in the POS tag for each word\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Format: (word, pos_tag)\n",
    "words_with_pos = [\n",
    "    (\"flying\", \"v\"),      # verb -> fly\n",
    "    (\"happily\", # TODO),   # adverb -> happily (adverbs don't change much)\n",
    "    (\"worse\", # TODO),     # adjective -> bad\n",
    "    (\"mice\", # TODO),      # noun -> mouse\n",
    "    (\"are\", # TODO),       # verb -> be\n",
    "]\n",
    "\n",
    "print(\"Lemmatization results:\")\n",
    "for word, pos in words_with_pos:\n",
    "    lemma = lemmatizer.lemmatize(word, pos=pos)\n",
    "    print(f\"  {word:12} ({pos}) -> {lemma}\")\n",
    "\n",
    "# Verify your answers\n",
    "expected = ['fly', 'happily', 'bad', 'mouse', 'be']\n",
    "results = [lemmatizer.lemmatize(w, pos=p) for w, p in words_with_pos]\n",
    "assert results == expected, f\"Expected {expected}, got {results}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Lemmatization with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCy automatically determines POS and lemmatizes correctly\n",
    "text = \"The children are playing with their toys. They were running and jumping happily.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "print(\"spaCy lemmatization (automatic POS detection):\")\n",
    "for token in doc:\n",
    "    if token.text != token.lemma_:  # Only show words that change\n",
    "        print(f\"  {token.text:12} ({token.pos_:5}) -> {token.lemma_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Exercise 2.3\n",
    "# Use spaCy to extract the lemmas of all non-punctuation tokens\n",
    "# Return as a list of lowercase lemmas\n",
    "\n",
    "text = \"The dogs were barking loudly at the cats who were climbing the trees.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "lemmas = # YOUR CODE HERE (list comprehension: token.lemma_.lower() for non-punct tokens)\n",
    "\n",
    "print(\"Lemmas:\", lemmas)\n",
    "assert lemmas == ['the', 'dog', 'be', 'bark', 'loudly', 'at', 'the', 'cat', 'who', 'be', 'climb', 'the', 'tree'], \"Check your lemmatization!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Stemming vs Lemmatization Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the two approaches\n",
    "words = [\"studies\", \"studying\", \"better\", \"feet\", \"ran\", \"easily\", \"fairly\", \"wolves\"]\n",
    "\n",
    "porter = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(f\"{'Word':<12} {'Stemmed':<12} {'Lemmatized':<12}\")\n",
    "print(\"-\" * 36)\n",
    "for word in words:\n",
    "    stemmed = porter.stem(word)\n",
    "    # For comparison, we'll use noun as default\n",
    "    lemmatized = lemmatizer.lemmatize(word)\n",
    "    print(f\"{word:<12} {stemmed:<12} {lemmatized:<12}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Differences:**\n",
    "- Stemming is faster but may produce non-words (\"studi\", \"easili\")\n",
    "- Lemmatization produces valid words but is slower\n",
    "- Lemmatization requires POS information for best results\n",
    "- Use stemming for speed (search engines), lemmatization for accuracy (chatbots, NLU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Stop Words and Special Characters\n",
    "\n",
    "Stop words are common words that usually don't carry much meaning (the, is, at, which, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Stop Words with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Get English stop words\n",
    "stop_words_nltk = set(stopwords.words('english'))\n",
    "\n",
    "print(f\"Number of NLTK stop words: {len(stop_words_nltk)}\")\n",
    "print(f\"\\nSample stop words: {list(stop_words_nltk)[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words from text\n",
    "text = \"The quick brown fox jumps over the lazy dog in the park\"\n",
    "tokens = word_tokenize(text.lower())\n",
    "\n",
    "# Filter out stop words\n",
    "filtered_tokens = [token for token in tokens if token not in stop_words_nltk]\n",
    "\n",
    "print(\"Original tokens:\", tokens)\n",
    "print(\"Without stop words:\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Exercise 3.1\n",
    "# Remove stop words from the following text and return the remaining tokens\n",
    "# Make sure to lowercase the text first!\n",
    "\n",
    "text = \"This is a sample sentence showing the removal of stop words from the text\"\n",
    "\n",
    "# Step 1: Lowercase and tokenize\n",
    "tokens = # YOUR CODE HERE\n",
    "\n",
    "# Step 2: Remove stop words\n",
    "filtered = # YOUR CODE HERE\n",
    "\n",
    "print(\"Filtered tokens:\", filtered)\n",
    "assert filtered == ['sample', 'sentence', 'showing', 'removal', 'stop', 'words', 'text'], \"Check your filtering!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Stop Words with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCy has built-in stop word detection\n",
    "text = \"This is a sample sentence for demonstrating stop word removal.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "print(\"Token analysis:\")\n",
    "for token in doc:\n",
    "    print(f\"  {token.text:<15} is_stop: {token.is_stop}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter using spaCy's is_stop attribute\n",
    "content_words = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
    "print(\"Content words:\", content_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize stop words in spaCy\n",
    "# Add custom stop words\n",
    "nlp.vocab[\"sample\"].is_stop = True\n",
    "\n",
    "# Remove words from stop list\n",
    "nlp.vocab[\"not\"].is_stop = False  # 'not' carries meaning!\n",
    "\n",
    "# Check the spaCy stop words list\n",
    "print(f\"Number of spaCy stop words: {len(nlp.Defaults.stop_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Removing Special Characters and Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# Python's string.punctuation\n",
    "print(\"Punctuation characters:\", string.punctuation)\n",
    "\n",
    "# Method 1: Using str.translate()\n",
    "text = \"Hello, World! How's it going? #NLP @user\"\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "clean_text = text.translate(translator)\n",
    "print(\"\\nUsing translate():\", clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Using regex\n",
    "import re\n",
    "\n",
    "text = \"Hello, World! How's it going? #NLP @user 123\"\n",
    "\n",
    "# Remove all non-alphanumeric characters (keep spaces)\n",
    "clean_text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "print(\"Regex (letters only):\", clean_text)\n",
    "\n",
    "# Remove punctuation but keep numbers\n",
    "clean_text2 = re.sub(r'[^\\w\\s]', '', text)\n",
    "print(\"Regex (alphanumeric):\", clean_text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: Using spaCy token attributes\n",
    "text = \"Hello! This is @user's tweet about #NLP. Check https://example.com!\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "# Filter tokens\n",
    "clean_tokens = [\n",
    "    token.text for token in doc \n",
    "    if not token.is_punct \n",
    "    and not token.is_space\n",
    "    and not token.like_url\n",
    "]\n",
    "\n",
    "print(\"Clean tokens:\", clean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Exercise 3.2\n",
    "# Clean the following text by:\n",
    "# 1. Removing URLs\n",
    "# 2. Removing mentions (@user)\n",
    "# 3. Removing hashtags (#topic)\n",
    "# 4. Removing punctuation\n",
    "# 5. Converting to lowercase\n",
    "# Use regex for this exercise\n",
    "\n",
    "text = \"Check out @OpenAI's new model! https://openai.com #AI #MachineLearning It's amazing!!!\"\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# Hint: Apply multiple re.sub() operations\n",
    "clean = text\n",
    "clean = # Remove URLs\n",
    "clean = # Remove mentions\n",
    "clean = # Remove hashtags\n",
    "clean = # Remove punctuation\n",
    "clean = # Lowercase\n",
    "clean = # Remove extra whitespace\n",
    "\n",
    "print(f\"Clean text: '{clean}'\")\n",
    "assert clean == \"check out new model its amazing\", f\"Got: '{clean}'\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Complete Preprocessing Pipeline\n",
    "\n",
    "Now let's combine everything into a complete text preprocessing pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Example Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pipeline_example(text):\n",
    "    \"\"\"\n",
    "    Example preprocessing pipeline.\n",
    "    \n",
    "    Steps:\n",
    "    1. Lowercase\n",
    "    2. Remove URLs\n",
    "    3. Remove special characters\n",
    "    4. Tokenize\n",
    "    5. Remove stop words\n",
    "    6. Lemmatize\n",
    "    \"\"\"\n",
    "    # Step 1: Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Step 2: Remove URLs\n",
    "    text = re.sub(r'https?://\\S+', '', text)\n",
    "    \n",
    "    # Step 3: Remove special characters (keep only letters and spaces)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # Step 4: Tokenize with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Step 5 & 6: Remove stop words and lemmatize\n",
    "    tokens = [\n",
    "        token.lemma_ \n",
    "        for token in doc \n",
    "        if not token.is_stop \n",
    "        and not token.is_punct \n",
    "        and not token.is_space\n",
    "        and len(token.text) > 1  # Remove single characters\n",
    "    ]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Test the pipeline\n",
    "sample_text = \"\"\"\n",
    "The quick brown foxes are jumping over the lazy dogs! \n",
    "Check out https://example.com for more information.\n",
    "This is SO amazing!!! #NLP #Python @user123\n",
    "\"\"\"\n",
    "\n",
    "result = preprocess_pipeline_example(sample_text)\n",
    "print(\"Preprocessed tokens:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Final Challenge: Build Your Own Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Exercise 4.1 (FINAL CHALLENGE)\n",
    "# Create a complete preprocessing pipeline function that:\n",
    "# 1. Converts text to lowercase\n",
    "# 2. Removes URLs (http/https)\n",
    "# 3. Removes email addresses\n",
    "# 4. Removes mentions (@user) and hashtags (#topic)\n",
    "# 5. Removes numbers\n",
    "# 6. Removes punctuation and special characters\n",
    "# 7. Tokenizes the text\n",
    "# 8. Removes stop words\n",
    "# 9. Applies lemmatization\n",
    "# 10. Removes tokens with less than 2 characters\n",
    "#\n",
    "# The function should return a list of cleaned tokens\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Complete text preprocessing pipeline.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Raw input text\n",
    "        \n",
    "    Returns:\n",
    "        list: List of preprocessed tokens\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Step 1: Lowercase\n",
    "    \n",
    "    # Step 2: Remove URLs\n",
    "    \n",
    "    # Step 3: Remove emails\n",
    "    \n",
    "    # Step 4: Remove mentions and hashtags\n",
    "    \n",
    "    # Step 5: Remove numbers\n",
    "    \n",
    "    # Step 6: Remove punctuation/special characters\n",
    "    \n",
    "    # Step 7: Tokenize (use spaCy)\n",
    "    \n",
    "    # Step 8 & 9: Remove stop words and lemmatize\n",
    "    \n",
    "    # Step 10: Remove short tokens\n",
    "    \n",
    "    pass  # Remove this line and return your tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your pipeline with this text\n",
    "test_text = \"\"\"\n",
    "ðŸš€ BREAKING NEWS!!! The researchers at @MIT have published 5 new papers \n",
    "about Natural Language Processing! Check out https://mit.edu/nlp for details.\n",
    "Contact them at research@mit.edu for collaborations. #NLP #AI #Research\n",
    "\n",
    "The experiments were conducted using state-of-the-art transformers models.\n",
    "They achieved 95.5% accuracy on the benchmark datasets!!!\n",
    "\"\"\"\n",
    "\n",
    "result = preprocess_text(test_text)\n",
    "print(\"Preprocessed tokens:\")\n",
    "print(result)\n",
    "\n",
    "# Verify some expected tokens are in the result\n",
    "expected_tokens = ['researcher', 'publish', 'paper', 'natural', 'language', 'processing']\n",
    "for token in expected_tokens:\n",
    "    assert token in result, f\"Expected '{token}' in result\"\n",
    "\n",
    "# Verify unwanted elements are NOT in result\n",
    "unwanted = ['@mit', 'https', 'mit.edu', '#nlp', '95.5', '!!!', 'the', 'a', 'at']\n",
    "for item in unwanted:\n",
    "    assert item not in result, f\"'{item}' should not be in result\"\n",
    "\n",
    "print(\"\\nâœ… All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Applying Pipeline to Multiple Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Exercise 4.2\n",
    "# Apply your preprocessing pipeline to a list of documents\n",
    "# Return a list of lists (one list of tokens per document)\n",
    "\n",
    "documents = [\n",
    "    \"Machine learning is transforming the tech industry! @Google #ML\",\n",
    "    \"I love programming in Python. It's so easy to learn! https://python.org\",\n",
    "    \"The cats are sleeping on the couch. They're so lazy!\",\n",
    "    \"Contact support@company.com for any questions about our AI products.\"\n",
    "]\n",
    "\n",
    "# Apply your pipeline to each document\n",
    "processed_docs = # YOUR CODE HERE (list comprehension)\n",
    "\n",
    "print(\"Processed documents:\")\n",
    "for i, doc in enumerate(processed_docs):\n",
    "    print(f\"  Doc {i+1}: {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Tokenization\n",
    "- **Basic**: `str.split()` - simple but limited\n",
    "- **NLTK**: `word_tokenize()`, `sent_tokenize()` - handles punctuation\n",
    "- **spaCy**: `nlp(text)` - provides rich token information\n",
    "- **Custom**: `RegexpTokenizer` - for specific patterns\n",
    "\n",
    "### Normalization\n",
    "- **Stemming**: Fast, rule-based (Porter, Snowball) - may produce non-words\n",
    "- **Lemmatization**: Accurate, vocabulary-based - produces valid words\n",
    "- **spaCy**: Automatic POS-aware lemmatization with `token.lemma_`\n",
    "\n",
    "### Filtering\n",
    "- **Stop words**: NLTK `stopwords.words()`, spaCy `token.is_stop`\n",
    "- **Punctuation**: `string.punctuation`, spaCy `token.is_punct`\n",
    "- **Special characters**: regex `re.sub()`\n",
    "\n",
    "### Pipeline Best Practices\n",
    "1. Order matters! (lowercase before regex, tokenize before lemmatize)\n",
    "2. Choose stemming vs lemmatization based on your task\n",
    "3. Consider what stop words to keep (e.g., \"not\" for sentiment)\n",
    "4. Test your pipeline on sample data\n",
    "\n",
    "---\n",
    "\n",
    "## Submission\n",
    "\n",
    "1. Make sure all exercises are completed\n",
    "2. Save this notebook\n",
    "3. Create a Git repository and push your work\n",
    "4. **Send the repository link to: yoroba93@gmail.com**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
